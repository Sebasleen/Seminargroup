{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b02ed9dd222dd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T14:59:00.870501Z",
     "start_time": "2024-02-26T14:59:00.868953Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import calendar\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0fca218438e6d",
   "metadata": {},
   "source": [
    "# Table 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7850c72b3792a",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aefa9a33ec9759a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T13:12:17.659382Z",
     "start_time": "2024-02-26T13:12:16.740681Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac' 'bab' 'cfp' 'cma' 'ep' 'hml' 'liq' 'ltrev' 'nsi' 'qmj' 'rmw' 'rvar'\n",
      " 'smb' 'strev' 'umd' 'glbab' 'glcma' 'glhml' 'glqmj' 'glrmw' 'glsmb'\n",
      " 'glumd']\n",
      "       year  month anomaly    ret   time  global\n",
      "0      1963      7      ac  2.170   42.0     0.0\n",
      "1      1963      8      ac -0.197   43.0     0.0\n",
      "2      1963      9      ac  0.600   44.0     0.0\n",
      "3      1963     10      ac  6.463   45.0     0.0\n",
      "4      1963     11      ac -2.260   46.0     0.0\n",
      "...     ...    ...     ...    ...    ...     ...\n",
      "10111  2019      8     umd  7.600  715.0     0.0\n",
      "10112  2019      9     umd -6.850  716.0     0.0\n",
      "10113  2019     10     umd  0.240  717.0     0.0\n",
      "10114  2019     11     umd -2.620  718.0     0.0\n",
      "10115  2019     12     umd -2.130  719.0     0.0\n",
      "\n",
      "[10116 rows x 6 columns]\n",
      "       year  month anomaly       ret   time  global\n",
      "10116  1987      2   glbab  2.236918  325.0     1.0\n",
      "10117  1987      3   glbab  1.828450  326.0     1.0\n",
      "10118  1987      4   glbab -5.521739  327.0     1.0\n",
      "10119  1987      5   glbab -0.513814  328.0     1.0\n",
      "10120  1987      6   glbab  1.579217  329.0     1.0\n",
      "...     ...    ...     ...       ...    ...     ...\n",
      "12638  2019      8   glumd  2.990000  715.0     1.0\n",
      "12639  2019      9   glumd -3.260000  716.0     1.0\n",
      "12640  2019     10   glumd -0.940000  717.0     1.0\n",
      "12641  2019     11   glumd  0.000000  718.0     1.0\n",
      "12642  2019     12   glumd  0.740000  719.0     1.0\n",
      "\n",
      "[2527 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/anomalies.dta'\n",
    "\n",
    "Anomalies = pd.read_stata(url)\n",
    "\n",
    "# display unique values in the 'anomaly' column\n",
    "print(Anomalies['anomaly'].unique())\n",
    "\n",
    "# delete the global factors from the dataframe and create Anomalies US\n",
    "column_name = 'anomaly'\n",
    "values_to_dropUS = ['glbab', 'glcma', 'glhml', 'glqmj', 'glrmw', 'glsmb', 'glumd']\n",
    "ElementsUS = Anomalies[column_name].isin(values_to_dropUS)\n",
    "Anomalies_US = Anomalies[~ElementsUS]\n",
    "\n",
    "# delete the US factors from dataframe and create Anomalies Global \n",
    "column_name = 'anomaly'\n",
    "values_to_dropGF = ['ac', 'bab', 'cfp', 'cma', 'ep', 'hml', 'liq', 'ltrev', 'nsi', 'qmj', 'rmw', 'rvar',\n",
    "                    'smb', 'strev', 'umd']\n",
    "ElementsGF = Anomalies[column_name].isin(values_to_dropGF)\n",
    "Anomalies_GF = Anomalies[~ElementsGF]\n",
    "\n",
    "# print both anomalies US and anomalies global \n",
    "print(Anomalies_US)\n",
    "print(Anomalies_GF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2b93f47c09c62",
   "metadata": {},
   "source": [
    "### replicating table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4c916e084c319",
   "metadata": {},
   "source": [
    "### US factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66347171583346c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T13:12:32.874029Z",
     "start_time": "2024-02-26T13:12:32.830780Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   anomaly  Mean     SD T-value\n",
      "0       ac  2.8%   6.6%    3.19\n",
      "1      bab  9.8%  11.2%    6.55\n",
      "2      cfp  3.4%   8.6%    2.94\n",
      "3      cma  3.3%   6.9%    3.59\n",
      "4       ep  3.5%   8.9%    2.95\n",
      "5      hml  3.6%   9.7%    2.82\n",
      "6      liq  4.4%  11.6%    2.77\n",
      "7    ltrev  2.5%   8.7%    2.16\n",
      "8      nsi  2.8%   8.2%    2.52\n",
      "9      qmj  4.6%   7.7%    4.47\n",
      "10     rmw  3.1%   7.5%    3.13\n",
      "11    rvar  1.6%  17.3%    0.68\n",
      "12     smb  2.7%  10.4%    1.97\n",
      "13   strev  6.0%  10.6%    4.21\n",
      "14     umd  7.8%  14.5%    4.02\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean and stand deviation of US factors\n",
    "AnomaliesUS = Anomalies_US.groupby(['anomaly']).agg({'ret': ['mean', 'std', 'count']}).reset_index()\n",
    "AnomaliesUS = Anomalies_US.pivot_table(index='anomaly', values='ret', aggfunc=['mean', 'std', 'count'])\n",
    "AnomaliesUS.columns = ['Mean', 'SD', 'ret_number']\n",
    "AnomaliesUS.reset_index(inplace=True)\n",
    "AnomaliesUS.columns = ['anomaly', 'Mean', 'SD', 'ret_number']\n",
    "\n",
    "# calculate additional statistics\n",
    "AnomaliesUS['ret_semean'] = AnomaliesUS['SD'] / np.sqrt(AnomaliesUS['ret_number'])\n",
    "\n",
    "# multiply by 12 to create annualized returns\n",
    "AnomaliesUS['ret'] = AnomaliesUS['Mean'] * 12\n",
    "\n",
    "# multiply by sqrt(12) to create annualized standard deviation\n",
    "AnomaliesUS['sd'] = AnomaliesUS['SD'] * np.sqrt(12)\n",
    "\n",
    "# calculate the t-stat by dividing the return by the standard error of the mean. Divide by 12 to annualize it\n",
    "AnomaliesUS['tstat'] = AnomaliesUS['ret'] / AnomaliesUS['ret_semean'] /12\n",
    "\n",
    "# format the table to correct decimals\n",
    "AnomaliesUS[['Mean']] = AnomaliesUS[['ret']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesUS[['SD']] = AnomaliesUS[['sd']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesUS[['T-value']] = AnomaliesUS[['tstat']].apply(lambda x: x.map(\"{:.2f}\".format))\n",
    "\n",
    "print(AnomaliesUS[['anomaly','Mean','SD','T-value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f970bbebcafdd90",
   "metadata": {},
   "source": [
    "### Global factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbd214b9e39f7b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T13:12:42.065180Z",
     "start_time": "2024-02-26T13:12:42.044718Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anomaly  Mean     SD T-value\n",
      "0   glbab  9.6%   9.7%    5.70\n",
      "1   glcma  1.9%   6.0%    1.74\n",
      "2   glhml  4.0%   7.4%    2.92\n",
      "3   glqmj  6.2%   6.8%    5.06\n",
      "4   glrmw  4.3%   4.7%    4.91\n",
      "5   glsmb  1.1%   7.1%    0.83\n",
      "6   glumd  7.9%  12.1%    3.54\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean and stand deviation of Global factors\n",
    "AnomaliesGF = Anomalies_GF.groupby(['anomaly']).agg({'ret': ['mean', 'std', 'count']}).reset_index()\n",
    "AnomaliesGF = Anomalies_GF.pivot_table(index='anomaly', values='ret', aggfunc=['mean', 'std', 'count'])\n",
    "AnomaliesGF.columns = ['Mean', 'SD', 'ret_number']\n",
    "AnomaliesGF.reset_index(inplace=True)\n",
    "AnomaliesGF.columns = ['anomaly', 'Mean', 'SD', 'ret_number']\n",
    "\n",
    "# calculate additional statistics\n",
    "AnomaliesGF['ret_semean'] = AnomaliesGF['SD'] / np.sqrt(AnomaliesGF['ret_number'])\n",
    "\n",
    "# multiply by 12 to create annualized returns\n",
    "AnomaliesGF['ret'] = AnomaliesGF['Mean'] * 12\n",
    "\n",
    "# multiply by sqrt(12) to create annualized standard deviation\n",
    "AnomaliesGF['sd'] = AnomaliesGF['SD'] * np.sqrt(12)\n",
    "\n",
    "# calculate the t-stat by dividing the return by the standard error of the mean. Divide by 12 to annualize it\n",
    "AnomaliesGF['tstat'] = AnomaliesGF['ret'] / AnomaliesGF['ret_semean'] /12\n",
    "\n",
    "# format the table to correct decimals\n",
    "AnomaliesGF[['Mean']] = AnomaliesGF[['ret']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesGF[['SD']] = AnomaliesGF[['sd']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesGF[['T-value']] = AnomaliesGF[['tstat']].apply(lambda x: x.map(\"{:.2f}\".format))\n",
    "\n",
    "print(AnomaliesGF[['anomaly','Mean','SD','T-value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16892a0e1af284",
   "metadata": {},
   "source": [
    "# Table 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5369eb282369a4",
   "metadata": {},
   "source": [
    "### replicating table 2 (uses the same dataset as table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d64fbe7d50f9ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T13:22:38.886376Z",
     "start_time": "2024-02-26T13:22:38.813337Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   anomaly     alpha  T-stat_alpha     slope  T-stat_slope\n",
      "0       ac  0.150195      1.184450  0.101410      0.649822\n",
      "1      bab -0.221412     -0.632211  1.319041      3.534152\n",
      "2      cfp  0.127745      0.781292  0.235454      1.157989\n",
      "3      cma  0.120082      0.974474  0.244693      1.545819\n",
      "4       ep  0.101357      0.616107  0.302075      1.458207\n",
      "5      hml  0.038477      0.204762  0.410255      1.780679\n",
      "6      liq  0.157215      0.741922  0.356063      1.291807\n",
      "7    ltrev -0.252989     -1.663307  0.757680      3.850110\n",
      "8      nsi  0.172982      1.324451  0.089249      0.486779\n",
      "9      qmj  0.086832      0.650364  0.434757      2.507550\n",
      "10     rmw  0.040360      0.222250  0.337185      1.673841\n",
      "11    rvar -0.463569     -1.638345  1.061609      2.737366\n",
      "12     smb -0.104191     -0.615583  0.583455      2.508982\n",
      "13   strev  0.485098      1.427336  0.013888      0.038600\n",
      "14     umd  0.716042      2.697340 -0.094969     -0.288098\n",
      "15   glbab  0.190820      0.577502  0.837610      2.303918\n",
      "16   glcma -0.064014     -0.408285  0.382064      1.944285\n",
      "17   glhml  0.035556      0.150752  0.471689      1.770057\n",
      "18   glqmj  0.394512      1.761234  0.124643      0.492025\n",
      "19   glrmw  0.137826      1.033410  0.256716      1.616411\n",
      "20   glsmb -0.063285     -0.388832  0.284797      1.325669\n",
      "21   glumd  0.668710      1.774142  0.017124      0.039403\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# create a loop which iterates over each anomly in our dataset \n",
    "for anomaly in Anomalies['anomaly'].unique():\n",
    "    subset = Anomalies[Anomalies['anomaly'] == anomaly]\n",
    "    subset = subset.sort_values(by='time')\n",
    "\n",
    "    # create a binary variable for positive returns in the past 12 months (the signal variable)\n",
    "    subset['positive_return'] = subset['ret'].rolling(window=12, min_periods=12).mean().shift(1) > 0\n",
    "\n",
    "    # drop the first 12 observations in the subset after the rolling window has been applied (so dropping N/A values)\n",
    "    subset = subset.iloc[12:]\n",
    "\n",
    "    # select our OLS model and fit our data\n",
    "    y = subset['ret']\n",
    "    X = sm.add_constant(subset['positive_return'].astype(int))\n",
    "    model = sm.OLS(y, X)\n",
    "    \n",
    "    # select the correct covariance type (as used in the paper)\n",
    "    results = model.fit(cov_type='cluster', cov_kwds={'groups': subset['time']})\n",
    "\n",
    "    # append the results to our dictionary to create the results\n",
    "    results_list.append({\n",
    "        'anomaly': anomaly,\n",
    "        'alpha': results.params['const'],\n",
    "        'T-stat_alpha': results.tvalues['const'],\n",
    "        'slope': results.params['positive_return'],\n",
    "        'T-stat_slope': results.tvalues['positive_return'],\n",
    "    })\n",
    "\n",
    "results_table = pd.DataFrame(results_list)\n",
    "print(results_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405d5744d09b948",
   "metadata": {},
   "source": [
    "# Table 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fe09105a82214",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b534a6439d1efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T13:43:12.562660Z",
     "start_time": "2024-02-26T13:43:06.532381Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# loading dataframes \n",
    "\n",
    "url = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/managed_portfolios_anom_d_55.csv'\n",
    "\n",
    "r_daily = pd.read_csv(url)\n",
    "\n",
    "# drop all momentum factors or factors that are constructed based on momentum (including market return variables)\n",
    "\n",
    "factor_drop_list = ['r_mom', 'r_indmom', 'r_valmom', 'r_valmomprof', 'r_mom12', 'r_momrev', 'r_indmomrev', 'r_exchsw', 'rme', 're_ew']\n",
    "\n",
    "r_daily.drop(columns=factor_drop_list, inplace=True)\n",
    "\n",
    "# set date to datetime format and set the date to the index \n",
    "\n",
    "r_daily['date'] = pd.to_datetime(r_daily['date'])\n",
    "r_daily.set_index('date', inplace=True)\n",
    "\n",
    "# following the procedure in the paper, if there are observations missing we set them to 0. (footnote 16)\n",
    "\n",
    "r_daily.fillna(0, inplace=True)\n",
    "\n",
    "# create a list of factors for later analysis purposes \n",
    "\n",
    "factors = [col for col in r_daily.columns if col.startswith('r_')]\n",
    "\n",
    "# create a monthly return dataframe for later analysis purposes (by summing the daily returns)\n",
    "\n",
    "r_monthly = r_daily.resample('m').sum()\n",
    "r_monthly.index = r_monthly.index.strftime('%Y-%m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6714174c41bb33",
   "metadata": {},
   "source": [
    "### perform the PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c93e32e30ad672c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T15:00:46.496892Z",
     "start_time": "2024-02-26T14:59:21.287217Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "date                                                                            \n",
      "1973-02 -0.163527  -0.10435 -0.004879  0.001906  0.080599   0.11734  0.004165   \n",
      "1973-03 -0.254274  -0.04289 -0.002538 -0.097974 -0.033213 -0.057495   0.00577   \n",
      "1973-04 -0.166303  0.053257  0.051769 -0.043656  0.022396  0.051989    0.0334   \n",
      "1973-05 -0.403385  0.076711   0.09642  -0.07831    0.0444  0.073014  0.079326   \n",
      "1973-06 -0.100178 -0.117113 -0.034034 -0.069168  0.000694  0.010336  0.061616   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "2019-09    0.0618  -0.26667   -0.0452  0.144064  0.059383 -0.041025  -0.05225   \n",
      "2019-10 -0.418177  0.059967  0.232239 -0.056775 -0.086553 -0.024549  0.072874   \n",
      "2019-11  0.035635 -0.109379  0.016161 -0.062541    0.0222  0.060263  0.078533   \n",
      "2019-12   0.20419  0.041358 -0.038417   0.01059 -0.027231  0.046026   0.01053   \n",
      "2020-01 -0.072692  0.050692  0.067176 -0.032406  0.022009 -0.003676  0.002061   \n",
      "\n",
      "              PC8       PC9      PC10  ...      PC38      PC39      PC40  \\\n",
      "date                                   ...                                 \n",
      "1973-02  0.003035 -0.079989 -0.020695  ... -0.021327  0.008742   -0.0051   \n",
      "1973-03  0.042633 -0.035161 -0.017398  ...  0.005376   -0.0003 -0.000367   \n",
      "1973-04  0.022461 -0.026694 -0.010816  ...  0.019176 -0.001799  0.003847   \n",
      "1973-05  0.073523 -0.055137  0.005008  ... -0.026462  0.000317 -0.007336   \n",
      "1973-06   0.12224  0.016014  0.004321  ... -0.006241 -0.000443  0.006889   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "2019-09   0.01717 -0.040238 -0.030897  ... -0.001199  0.008439 -0.021196   \n",
      "2019-10 -0.016393  0.047308  0.047244  ...  0.010715  0.004885  0.001281   \n",
      "2019-11  -0.04381  0.036236   0.03833  ...  0.005283 -0.005292  0.014402   \n",
      "2019-12  0.021675 -0.026845  0.012445  ...  0.001963 -0.005495  0.007557   \n",
      "2020-01 -0.008543  0.032947  0.011972  ...  0.011556  0.004658 -0.006268   \n",
      "\n",
      "             PC41      PC42      PC43      PC44      PC45      PC46      PC47  \n",
      "date                                                                           \n",
      "1973-02 -0.010304 -0.007092  0.008458  0.010323       0.0       0.0       0.0  \n",
      "1973-03 -0.015084  0.006569  -0.00155  0.014107       0.0       0.0       0.0  \n",
      "1973-04 -0.005882  -0.00524  0.001282  0.011108      -0.0      -0.0       0.0  \n",
      "1973-05 -0.026076 -0.004754 -0.014463  0.021661      -0.0       0.0       0.0  \n",
      "1973-06  0.008113  0.004878  0.007792  0.006805       0.0       0.0       0.0  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "2019-09 -0.000517 -0.006274  0.003448  0.001092 -0.006518 -0.004247   -0.0076  \n",
      "2019-10  0.004942  -0.00468 -0.000161  0.000391 -0.008263  0.006965  0.001521  \n",
      "2019-11 -0.001159  0.012408  0.004936  0.003515 -0.006954 -0.000451  0.000488  \n",
      "2019-12 -0.002737  0.009973  0.009825  0.004672  0.001641 -0.011689 -0.006748  \n",
      "2020-01  0.010561 -0.001968 -0.003344 -0.006232  0.000492  0.005839  0.001024  \n",
      "\n",
      "[564 rows x 47 columns]\n",
      "r_size        -0.000682\n",
      "r_value       -0.331981\n",
      "r_prof         0.289461\n",
      "r_dur         -0.036173\n",
      "r_valprof     -0.055813\n",
      "r_fscore       0.010171\n",
      "r_debtiss      0.015000\n",
      "r_repurch      0.008877\n",
      "r_nissa       -0.003174\n",
      "r_accruals     0.001457\n",
      "r_growth       0.001918\n",
      "r_aturnover   -0.025761\n",
      "r_gmargins    -0.089170\n",
      "r_divp         0.001507\n",
      "r_ep          -0.005460\n",
      "r_cfp          0.015275\n",
      "r_noa          0.006048\n",
      "r_inv         -0.005012\n",
      "r_invcap       0.029903\n",
      "r_igrowth      0.003919\n",
      "r_sgrowth      0.001356\n",
      "r_lev          0.802271\n",
      "r_roaa         0.183349\n",
      "r_roea        -0.162525\n",
      "r_sp          -0.287100\n",
      "r_gltnoa      -0.003443\n",
      "r_divg         0.004544\n",
      "r_invaci       0.000135\n",
      "r_shortint     0.010073\n",
      "r_lrrev        0.004674\n",
      "r_valuem       0.020405\n",
      "r_nissm       -0.004031\n",
      "r_sue         -0.004642\n",
      "r_roe         -0.027740\n",
      "r_rome         0.029437\n",
      "r_roa          0.003879\n",
      "r_strev        0.006121\n",
      "r_ivol         0.045911\n",
      "r_betaarb     -0.017066\n",
      "r_season       0.001686\n",
      "r_indrrev     -0.010167\n",
      "r_indrrevlv    0.003910\n",
      "r_ciss        -0.002259\n",
      "r_price        0.018676\n",
      "r_age          0.005248\n",
      "r_shvol       -0.000707\n",
      "r_ipo          0.001396\n",
      "Name: PC47, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# initialize pca model \n",
    "\n",
    "pca = PCA(n_components=len(factors))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# select our start date \n",
    "\n",
    "start_date = pd.to_datetime(\"1963-07-01\")\n",
    "\n",
    "# create an empty dataframe to store the average return for each PC from t until t-11. We need this to create the momentum signal for our strategy\n",
    "\n",
    "pc_avg_df = pd.DataFrame()\n",
    "\n",
    "# create an empty list for the pc return dataframes. These will be concated in a later stage to one large dataframe\n",
    "\n",
    "pc_return_dfs = []\n",
    "\n",
    "# create our loop set up, this is actually an expanding PCA analysis. In each iteration a new month is added to the dataset and the return is computed. \n",
    "\n",
    "for year in range(1973, 2020):\n",
    "    # the sample of the paper starts from July 1973, but we use January to June 1973 to calculate the returns in order to obtain stable means for later demeaning purposes \n",
    "    for mo in range(1,13):\n",
    "        # first we have to find the last month of the day. For this we use the calender function with inputs from the loop variables\n",
    "        last_day = calendar.monthrange(year, mo)[1]\n",
    "\n",
    "        # we select our new end_date variable for which the PCA analysis is done, also with inputs from our loop and the last_day variable\n",
    "        end_date = pd.to_datetime(f'{year}-{mo}-{last_day}')\n",
    "\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "\n",
    "        # we select the datarange from our dataset (July 1963 = start_date until our defined end_date) and we fit the model\n",
    "        pca_data = r_daily.loc[start_date:end_date]\n",
    "        scaled_data = scaler.fit_transform(pca_data)\n",
    "        pca.fit(scaled_data)\n",
    "\n",
    "        # we extract the principal components. These principal components are put in a new dataframe for later analysis. \n",
    "\n",
    "        principal_components = pca.components_\n",
    "        components_df = pd.DataFrame(data=principal_components.T, index=factors, columns=[f\"PC{i+1}\" for i in range(len(factors))])\n",
    "\n",
    "        # calculating return for month t+1. If mo = 12, then year will increment with 1. \n",
    "\n",
    "        t_plus_1_year = year + 1 if mo == 12 else year\n",
    "        t_plus_1_month = (mo % 12) + 1\n",
    "\n",
    "        # creating a datetime variable for the month t+1 and storing this in our pc_return_data variable\n",
    "\n",
    "        t_plus_1_dt =pd.to_datetime(f'{t_plus_1_year}-{t_plus_1_month}')\n",
    "        t_plus_1 = t_plus_1_dt.strftime('%Y-%m')\n",
    "\n",
    "        pc_return_data = {'date': t_plus_1}\n",
    "\n",
    "\n",
    "        # in this loop we calculate the monthly factor returns (f) using the principal components and returns\n",
    "\n",
    "        for f in range(len(factors)):\n",
    "            # select our factor and extract its principal component from principal_df and its return from r_daily for all observations in month mo \n",
    "            pc = components_df.iloc[:, f]\n",
    "            r_month = r_monthly.loc[t]\n",
    "            # multiply the principal components with the returns and sum them up to get PC factor return for month mo \n",
    "            pc_return = (pc*r_month).sum()\n",
    "\n",
    "            # place this in our dictionary for later transposing to dataframe\n",
    "            pc_return_data[components_df.columns[f]] = pc_return\n",
    "\n",
    "            r_pc_month_n_list = []\n",
    "            \n",
    "            # in this loop we calculate the average return of the eigenvector at time t, for the period t until t-11. We store these results in a dataframe for later use.\n",
    "\n",
    "            for n in range(0, 12):\n",
    "                # calculate the datetime for t - n\n",
    "                t_minus_n_dt = t_dt - pd.DateOffset(months=n)\n",
    "\n",
    "                # transpose it to our YYYY-MM format\n",
    "                t_minus_n = t_minus_n_dt.strftime('%Y-%m')\n",
    "\n",
    "                # select the return corresponding to our month t-n\n",
    "                r_month_n = r_monthly.shift(n).loc[t_minus_n]\n",
    "\n",
    "                # calculate the dot product for month t-n\n",
    "                pc_return_n = (pc*r_month_n).sum()\n",
    "                \n",
    "                # append this to our list to calculate the mean \n",
    "                r_pc_month_n_list.append(pc_return_n)\n",
    "            \n",
    "            # calculate the mean and append it to our average return dataframe\n",
    "            r_pc_month_mean = (np.mean(r_pc_month_n_list))\n",
    "            pc_avg_df.loc[t, f'PC{f+1}'] = r_pc_month_mean\n",
    "\n",
    "        # append the PC returns to our PC_return dataframe for later analysis \n",
    "        pc_return_df = pd.DataFrame.from_dict(pc_return_data, orient='index').T\n",
    "        pc_return_df.set_index('date', inplace=True)\n",
    "        pc_return_dfs.append(pc_return_df)\n",
    "\n",
    "r_pc = pd.concat(pc_return_dfs)\n",
    "print(r_pc)\n",
    "print(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de35b475e77b4",
   "metadata": {},
   "source": [
    "### demeaning and leveraging our PC returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60e65f621120396c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T15:13:06.527352Z",
     "start_time": "2024-02-26T15:12:59.594681Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m demeaned_r_pc_t \u001b[38;5;241m=\u001b[39m r_pc\u001b[38;5;241m.\u001b[39mloc[t]\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m r_pc_t\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# calculate the leverage factor and multiply this with the demeaned \u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m leverage_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(np\u001b[38;5;241m.\u001b[39msqrt(avg_var_indiv_f_t), r_pc_t\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), where\u001b[38;5;241m=\u001b[39mr_pc_t\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     34\u001b[0m lev_r_pc_t \u001b[38;5;241m=\u001b[39m demeaned_r_pc_t \u001b[38;5;241m*\u001b[39m leverage_t\n\u001b[1;32m     35\u001b[0m lev_df \u001b[38;5;241m=\u001b[39m lev_r_pc_t\u001b[38;5;241m.\u001b[39mloc[t]\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:2102\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_ufunc__\u001b[39m(\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28mself\u001b[39m, ufunc: np\u001b[38;5;241m.\u001b[39mufunc, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   2101\u001b[0m ):\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arraylike\u001b[38;5;241m.\u001b[39marray_ufunc(\u001b[38;5;28mself\u001b[39m, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:396\u001b[0m, in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# ufunc(series, ...)\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(extract_array(x, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[0;32m--> 396\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ufunc, method)(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# ufunc(dataframe)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;66;03m# for np.<ufunc>(..) calls\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;66;03m# kwargs cannot necessarily be handled block-by-block, so only\u001b[39;00m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;66;03m# take this path if there are no kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:2102\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_ufunc__\u001b[39m(\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28mself\u001b[39m, ufunc: np\u001b[38;5;241m.\u001b[39mufunc, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   2101\u001b[0m ):\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arraylike\u001b[38;5;241m.\u001b[39marray_ufunc(\u001b[38;5;28mself\u001b[39m, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:396\u001b[0m, in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# ufunc(series, ...)\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(extract_array(x, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[0;32m--> 396\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ufunc, method)(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# ufunc(dataframe)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;66;03m# for np.<ufunc>(..) calls\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;66;03m# kwargs cannot necessarily be handled block-by-block, so only\u001b[39;00m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;66;03m# take this path if there are no kwargs\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: NDFrame.__array_ufunc__ at line 2102 (3321 times), array_ufunc at line 396 (3320 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:396\u001b[0m, in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# ufunc(series, ...)\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(extract_array(x, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[0;32m--> 396\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ufunc, method)(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# ufunc(dataframe)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;66;03m# for np.<ufunc>(..) calls\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;66;03m# kwargs cannot necessarily be handled block-by-block, so only\u001b[39;00m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;66;03m# take this path if there are no kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:2102\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_ufunc__\u001b[39m(\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28mself\u001b[39m, ufunc: np\u001b[38;5;241m.\u001b[39mufunc, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   2101\u001b[0m ):\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arraylike\u001b[38;5;241m.\u001b[39marray_ufunc(\u001b[38;5;28mself\u001b[39m, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:273\u001b[0m, in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m _standardize_out_kwarg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# for binary ops, use our custom dunder methods\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m result \u001b[38;5;241m=\u001b[39m maybe_dispatch_ufunc_to_dunder_op(\u001b[38;5;28mself\u001b[39m, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32mops_dispatch.pyx:89\u001b[0m, in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "# define our start date and create an empty list for our leveraged dataframes\n",
    "start_date_dt = pd.to_datetime(\"1963-07-01\")\n",
    "start_date = start_date_dt.strftime('%Y-%m')\n",
    "lev_dfs = []\n",
    "\n",
    "for year in range(1973, 2020):\n",
    "    # as we lost one month in calculating the t+1 PC return, we start the loop from february 1973. We try to use the full year of 1973 in order to obtain stable demeaned results. Hence we will cut of our sample later from July 1973 in order to match the dataset of the original paper. \n",
    "    for mo in range(2,13) if year == 1973 else range(1, 13):\n",
    "        \n",
    "        # first we set our t variable to the current year and month from our loop\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "        \n",
    "        # we also create a t_minus_one variable, because we have to calculate the variance up to month t (so excluding month t)\n",
    "        t_minus_one_dt = t_dt - pd.DateOffset(months=1)\n",
    "        t_minus_one = t_minus_one_dt.strftime('%Y-%m')\n",
    "\n",
    "        # calculate the variance of the individual factor returns up until month t-1 \n",
    "        r_indiv_f_t = r_monthly.loc[start_date:t_minus_one]\n",
    "        var_indiv_f_t = r_indiv_f_t.var(axis=0)\n",
    "        avg_var_indiv_f_t = var_indiv_f_t.mean()\n",
    "\n",
    "        # calculate the mean and variance of the PC factors up until month t \n",
    "        r_pc_t = r_pc.loc[:t]\n",
    "        demeaned_r_pc_t = r_pc.loc[t].to_frame().T - r_pc_t.mean()\n",
    "\n",
    "        # calculate the leverage factor and multiply this with the demeaned \n",
    "\n",
    "        leverage_t = np.divide(np.sqrt(avg_var_indiv_f_t), r_pc_t.std(axis=0), where=r_pc_t.std(axis=0)!=0)\n",
    "        lev_r_pc_t = demeaned_r_pc_t * leverage_t\n",
    "        lev_df = lev_r_pc_t.loc[t].to_frame().T\n",
    "        lev_dfs.append(lev_df)\n",
    "\n",
    "lev_r_pc = pd.concat(lev_dfs)\n",
    "lev_r_pc.fillna(0, inplace=True)\n",
    "lev_r_pc_clean = lev_r_pc.drop(lev_r_pc.index[:1])\n",
    "print(lev_r_pc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c254e55de9a83",
   "metadata": {},
   "source": [
    "### constructing the momentum strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8716302d332df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T15:14:22.768475Z",
     "start_time": "2024-02-26T15:14:22.739005Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create two boolean dataframes: one for positive average returns and one for negative average returns\n",
    "positive_returns_PC = pc_avg_df > 0\n",
    "negative_returns_PC = pc_avg_df < 0\n",
    "\n",
    "# convert the boolean dataframes to integers and 0's, one for long positions and one for short positions\n",
    "long_portfolio_PC = positive_returns_PC.astype(int)\n",
    "short_portfolio_PC = negative_returns_PC.astype(int)\n",
    "\n",
    "# create the 5 subsets of PCs\n",
    "mom_1_10 = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10']\n",
    "mom_11_20 = ['PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20']\n",
    "mom_21_30 = ['PC21', 'PC22', 'PC23', 'PC24', 'PC25', 'PC26', 'PC27', 'PC28', 'PC29', 'PC30']\n",
    "mom_31_40 = ['PC31', 'PC32', 'PC33', 'PC34', 'PC35', 'PC36', 'PC37', 'PC38', 'PC39', 'PC40']\n",
    "mom_41_47 = ['PC41', 'PC42', 'PC43', 'PC44', 'PC45', 'PC46', 'PC47']\n",
    "\n",
    "# create a list of the subsets for our loop\n",
    "mom_list = [mom_1_10, mom_11_20, mom_21_30, mom_31_40, mom_41_47]\n",
    "\n",
    "# create an empty dictionary \n",
    "r_mean_set_dict = {}\n",
    "\n",
    "# create a loop where the dummy dataframe is multiplied with the leveraged PC return dataframe. We shift the portfolio indicator with one, as we need to calculate the returns of t+1. \n",
    "for i, mom in enumerate(mom_list):\n",
    "    # create the strategy: the return of the long positions minus the return of the short positions (accounting for the fact that negative short returns need to become positive\n",
    "    r_PC_set_mom = (long_portfolio_PC[mom] * lev_r_pc_clean[mom]) - (short_portfolio_PC[mom] * lev_r_pc_clean[mom])\n",
    "    # we take the mean of the returns of the 10 PC subsets\n",
    "    r_PC_set_mean = r_PC_set_mom.mean(axis=1)\n",
    "    # we append it to our dictionary\n",
    "    r_mean_set_dict[f'mom_set_{i + 1}'] = r_PC_set_mean\n",
    "\n",
    "# create the dataframe with the series of returns for each subset of PCS\n",
    "mom_strategy = pd.concat(r_mean_set_dict, axis=1)\n",
    "\n",
    "mom_strategy.index = pd.to_datetime(mom_strategy.index)\n",
    "mom_strategy.index = mom_strategy.index.strftime('%Y-%m')\n",
    "mom_strategy.dropna(inplace=True)\n",
    "\n",
    "print(mom_strategy.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233882ce4ca1ad2",
   "metadata": {},
   "source": [
    "### Replicating table 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971881e95f3633ce",
   "metadata": {},
   "source": [
    "### replicating panel A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77459761ad29a9d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T16:04:32.640240Z",
     "start_time": "2024-02-26T16:04:32.611455Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# select the full sample dataframe in the paper and create the two splitted periods \n",
    "mom_strategy_full = mom_strategy.loc['1973-07':'2019-12']\n",
    "mom_strategy_1 = mom_strategy.loc['1973-07':'1996-09']\n",
    "mom_strategy_2 = mom_strategy.loc['1996-09':]\n",
    "\n",
    "print(mom_strategy_full)\n",
    "\n",
    "print(f'the mean of every subset of PCs is:\\n')\n",
    "print(mom_strategy_full.mean(axis=0))\n",
    "\n",
    "means = mom_strategy.mean(axis=0).tolist()\n",
    "std = mom_strategy.std(axis=0).tolist()\n",
    "N = mom_strategy.shape[0]\n",
    "\n",
    "print(f'the t-statistic of every subset of PCS is:\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)\n",
    "\n",
    "print(f'the mean of every subset of PCs is (first half):\\n')\n",
    "print(mom_strategy_1.mean(axis=0))\n",
    "\n",
    "means = mom_strategy_1.mean(axis=0).tolist()\n",
    "std = mom_strategy_1.std(axis=0).tolist()\n",
    "N = mom_strategy_1.shape[0]\n",
    "\n",
    "print(f'the t-statistic of every subset of PCS is (first half):\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)\n",
    "\n",
    "print(f'the mean of every subset of PCs is (second half):\\n')\n",
    "print(mom_strategy_2.mean(axis=0))\n",
    "\n",
    "means = mom_strategy_2.mean(axis=0).tolist()\n",
    "std = mom_strategy_2.std(axis=0).tolist()\n",
    "N = mom_strategy_2.shape[0]\n",
    "\n",
    "print(f'the t-statistic of every subset of PCS is (second half):\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690983981c7aa12e",
   "metadata": {},
   "source": [
    "### Replicating panel B and C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbd2787c86808f",
   "metadata": {},
   "source": [
    "### Loading data and merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76edbfe897d967b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T16:04:47.458388Z",
     "start_time": "2024-02-26T16:04:47.208839Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/fffactors.dta'\n",
    "\n",
    "ff = pd.read_stata(url)\n",
    "\n",
    "# set index to date column\n",
    "ff.set_index('yyyymm', inplace=True)\n",
    "\n",
    "# set it to datetime format and correct format\n",
    "ff.index = pd.to_datetime(ff.index, format='%Y%m')\n",
    "ff.index = ff.index.strftime('%Y-%m')\n",
    "\n",
    "# select the needed factors over our sample \n",
    "ff5 = ff[['mktrf', 'smb', 'hml', 'rmw', 'cma']].loc['1973-07':'2019-12']\n",
    "\n",
    "# merge the dataframes together\n",
    "mom_strategy_ff5 = pd.concat([mom_strategy_full, ff5], axis=1)\n",
    "\n",
    "# construct a dummy variable for period 1 and for period 2 \n",
    "mom_strategy_ff5['P1'] = 0\n",
    "mom_strategy_ff5['P2'] = 0\n",
    "mom_strategy_ff5.loc[mom_strategy_ff5.index <= '1996-09', 'P1'] = 1\n",
    "mom_strategy_ff5.loc[mom_strategy_ff5.index >= '1996-09', 'P2'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366d0b552fada26",
   "metadata": {},
   "source": [
    "### conducting the regressions - Panel B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe1ed61f7d1b9e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# selecting our independent variables from our dataframe\n",
    "independent_vars = ['mom_set_1', 'P1', 'P2', 'mktrf', 'smb', 'hml', 'rmw', 'cma']\n",
    "X = mom_strategy_ff5[independent_vars]\n",
    "\n",
    "# constructing a loop that iterates the regressions\n",
    "for i in range(2, 6):\n",
    "    Y = mom_strategy_ff5[f'mom_set_{i}']\n",
    "    modelB = sm.OLS(Y, X, hasconst=False).fit()\n",
    "    print(modelB.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5a0f8ede0f950",
   "metadata": {},
   "source": [
    "### conducting the regressions - Panel C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453e2f179407de9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# selecting our dependent variable\n",
    "y = mom_strategy_ff5['mom_set_1']\n",
    "\n",
    "# constructing a loop that iterates the regressions\n",
    "for i in range(2, 6):\n",
    "    independent_vars = [f'mom_set_{i}', 'P1', 'P2', 'mktrf', 'smb', 'hml', 'rmw', 'cma']\n",
    "    X = mom_strategy_ff5[independent_vars]\n",
    "    modelC = sm.OLS(y, X, hasconst=False).fit()\n",
    "    print(modelC.summary())\n",
    "\n",
    "# last regression is one with all independent variables (panel C regression 5)\n",
    "independent_vars = ['P1', 'P2', 'mom_set_2', 'mom_set_3', 'mom_set_4', 'mom_set_5', 'mktrf', 'smb', 'hml', 'rmw', 'cma']\n",
    "X = mom_strategy_ff5[independent_vars]\n",
    "modelC1 = sm.OLS(y, X, hasconst=False).fit()\n",
    "print(modelC1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c465e1f-8c7c-4564-b766-9b4f7a2d162a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PLS Table 3 Panel A ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164f338-c8e3-46ff-a604-5d90241f3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/managed_portfolios_anom_d_55.csv'\n",
    "\n",
    "r_daily = pd.read_csv(url)\n",
    "momentum_list = ['r_mom', 'r_indmom', 'r_valmom', 'r_valmomprof', 'r_mom12', 'r_momrev', 'r_indmomrev', 'r_exchsw']\n",
    "r_daily.drop(columns=momentum_list, inplace=True)\n",
    "\n",
    "# set date to datetime format and set the date to the index \n",
    "r_daily['date'] = pd.to_datetime(r_daily['date'])\n",
    "r_daily.set_index('date', inplace=True)\n",
    "\n",
    "# following the procedure in the paper, if there are observations missing we set them to 0. \n",
    "r_daily.fillna(0, inplace=True)\n",
    "\n",
    "# create a list of factors for later analysis purposes \n",
    "factors = [col for col in r_daily.columns if col.startswith('r_')]\n",
    "r_daily.drop(columns=['rme', 're_ew'], inplace=True)\n",
    "\n",
    "# create a monthly return dataframe for later analysis purposes \n",
    "r_monthly = r_daily.resample(\"M\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53305c25-2b94-48e1-a5c2-8440fdce9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first calculate the average return of each factor in the period t until t-11 (and drop our missing year)\n",
    "r_monthly_average = r_monthly.rolling(window=12).mean()\n",
    "r_monthly_average.dropna(inplace=True)\n",
    "\n",
    "# create a boolean dataframe with True or False (True for positive returns or True for negative returns)\n",
    "positive_returns = r_monthly_average > 0\n",
    "negative_returns = r_monthly_average < 0\n",
    "\n",
    "# create from the boolean dataframe the binary dataframe \n",
    "long_portfolio = positive_returns.astype(int)\n",
    "short_portfolio = negative_returns.astype(int)\n",
    "\n",
    "# shift 1 month ahead to calculate the return of strategy at t+1 \n",
    "long_portfolio = long_portfolio.shift(1)\n",
    "short_portfolio = short_portfolio.shift(1)\n",
    "\n",
    "# reindex the long_portfolio dataframe to the daily frequency and the missing observations\n",
    "long_portfolio_daily = long_portfolio.reindex(r_daily.index)\n",
    "long_portfolio_daily = long_portfolio_daily.bfill()\n",
    "long_portfolio_daily = long_portfolio_daily.loc['1964-07-01':]\n",
    "\n",
    "# reindex the short_portfolio dataframe to the daily frequency and fill the missing observations\n",
    "\n",
    "short_portfolio_daily = short_portfolio.reindex(r_daily.index)\n",
    "short_portfolio_daily = short_portfolio_daily.bfill()\n",
    "short_portfolio_daily = short_portfolio_daily.loc['1964-07-01':]\n",
    "\n",
    "# create dataframe for returns that matches length of portfolio\n",
    "\n",
    "r_daily_strategy = r_daily.loc['1964-07-01':]\n",
    "\n",
    "# compute the return of the momentum strategy (this will be used as Y variable)\n",
    "r_strategy = ((r_daily_strategy * long_portfolio_daily) - (r_daily_strategy * short_portfolio_daily)).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3cca9-8b01-4eac-9b1f-05b5cb924599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first set our r_monthly back to expected YYYY-MM format \n",
    "r_monthly.index = r_monthly.index.strftime('%Y-%m')\n",
    "\n",
    "# select our start date \n",
    "start_date = pd.to_datetime(\"1964-07-01\")\n",
    "\n",
    "# create an empty dataframe for our calculations of the average return over the period t-1 to t-11\n",
    "pls_avg_df = pd.DataFrame()\n",
    "\n",
    "# create an empty list for the pls return dataframes. These will be concated in a later stage to one large dataframe\n",
    "pls_return_dfs = []\n",
    "\n",
    "# Create an empty dataframe for monthly PLS returns\n",
    "pls_return_df = pd.DataFrame(columns=['date'] + [f\"PLS{i+1}\" for i in range(len(factors))])\n",
    "\n",
    "# Initialize an empty list to store PLS weights\n",
    "pls_weights_list = []\n",
    "\n",
    "# Loop through years and months\n",
    "for year in range(1973, 2020):\n",
    "    # as our out of sample procedure starts July 1973, we start in the 7th month in 1973\n",
    "    for mo in range(2,13) if year == 1973 else range(1, 13):\n",
    "        # first we have to find the last month of the day. For this we use the calender function with inputs from the loop variables\n",
    "        last_day = calendar.monthrange(year, mo)[1]\n",
    "\n",
    "        # we select our new end_date variable for which the Pls analysis is done, also with inputs from our loop and the last_day variable\n",
    "        end_date = pd.to_datetime(f'{year}-{mo}-{last_day}')\n",
    "\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "\n",
    "        # select data range for PLS analysis\n",
    "        pls_data = r_daily.loc[start_date:end_date]\n",
    "        pls_data_y = r_strategy[start_date:end_date]\n",
    "\n",
    "        # assign features (X) and target (y)\n",
    "        X = pls_data\n",
    "        y = pls_data_y\n",
    "\n",
    "        # fit PLS model\n",
    "        pls = PLSRegression(n_components=47)\n",
    "        pls.fit(X, y)\n",
    "\n",
    "\n",
    "        # extract and store PLS weights\n",
    "        pls_weights = pd.DataFrame(\n",
    "            np.array(pls.x_weights_),\n",
    "            index=factors,\n",
    "            columns=[f\"PLS{i+1}\" for i in range(47)]\n",
    "        )\n",
    "        # calculating return for month t+1. If mo = 12, then year will increment with 1. \n",
    "\n",
    "        t_plus_1_year = year + 1 if mo == 12 else year\n",
    "        t_plus_1_month = (mo % 12) + 1\n",
    "\n",
    "        # creating a datetime variable for the month t+1 and storing this in our pls_return_data variable\n",
    "\n",
    "        t_plus_1_dt =pd.to_datetime(f'{t_plus_1_year}-{t_plus_1_month}')\n",
    "        t_plus_1 = t_plus_1_dt.strftime('%Y-%m')\n",
    "\n",
    "        pls_return_data = {'date': t_plus_1}\n",
    "\n",
    "\n",
    "        # in this loop we calculate the monthly factor returns (f) using the principal components and returns\n",
    "\n",
    "        for f in range(len(factors)):\n",
    "            # select our factor and extract its principal component from principal_df and its return from r_daily for all observations in month mo \n",
    "            pls = pls_weights.iloc[:, f]\n",
    "            r_month = r_monthly.loc[t]\n",
    "\n",
    "            # multiply the principal components with the returns and sum them up to get Pls factor return for month mo \n",
    "            pls_return = (pls*r_month).sum()\n",
    "\n",
    "            # place this in our dictionary for later transposing to dataframe\n",
    "\n",
    "            pls_return_data[pls_weights.columns[f]] = pls_return\n",
    "\n",
    "            r_pls_month_n_list = []\n",
    "\n",
    "            for n in range(1, 12):\n",
    "                # calculate the datetime for t - n\n",
    "                t_minus_n_dt = t_dt - pd.DateOffset(months=n)\n",
    "\n",
    "                # transpose it to our YYYY-MM format\n",
    "                t_minus_n = t_minus_n_dt.strftime('%Y-%m')\n",
    "\n",
    "                # select the return corresponding to our month t-n\n",
    "                r_month_n = r_monthly.shift(n).loc[t_minus_n]\n",
    "                pls_return_n = (pls*r_month_n).sum()\n",
    "                r_pls_month_n_list.append(pls_return_n)\n",
    "\n",
    "            r_pls_month_mean = (np.mean(r_pls_month_n_list))\n",
    "            pls_avg_df.loc[t, f'PLS{f+1}'] = r_pls_month_mean\n",
    "\n",
    "\n",
    "        pls_return_df = pd.DataFrame.from_dict(pls_return_data, orient='index').T\n",
    "        pls_return_df.set_index('date', inplace=True)\n",
    "        pls_return_dfs.append(pls_return_df)\n",
    "\n",
    "r_pls = pd.concat(pls_return_dfs)\n",
    "print(r_pls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28dd827-74a4-4239-968e-e345eb4ce893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start date\n",
    "start_date_dt = pd.to_datetime(\"1964-07-01\")\n",
    "start_date = start_date_dt.strftime('%Y-%m')\n",
    "\n",
    "# Create an empty list to store leverage-adjusted PLS factors\n",
    "lev_pls_dfs = []\n",
    "\n",
    "for year in range(1973, 2020):\n",
    "    # Loop through months\n",
    "    for mo in range(6, 13) if year == 1973 else range(1, 13):\n",
    "        # Define the current date\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "\n",
    "        # Calculate the variance of the individual factor returns up until month t \n",
    "        r_indiv_f_t = r_monthly.loc[start_date:t]\n",
    "        var_indiv_f_t = r_indiv_f_t.var(axis=0)\n",
    "        avg_var_indiv_f_t = var_indiv_f_t.mean()\n",
    "\n",
    "        # Calculate the mean and variance of the PLS factors up until month t \n",
    "        r_pls_t = r_pls.loc[:t]\n",
    "        demeaned_r_pls_t = r_pls_t.loc[t].to_frame().T - r_pls_t.mean()\n",
    "\n",
    "        # Calculate the leverage factor\n",
    "        nonzero_std = np.where(r_pls_t.std(axis=0) != 0, r_pls_t.std(axis=0), 1)\n",
    "        leverage_t = np.sqrt(avg_var_indiv_f_t) / nonzero_std\n",
    "\n",
    "        # Multiply leverage factor with the demeaned PLS factors\n",
    "        lev_r_pls_t = demeaned_r_pls_t * leverage_t\n",
    "\n",
    "        # Append the leveraged PLS factors to the list\n",
    "        lev_pls_dfs.append(lev_r_pls_t)\n",
    "\n",
    "# Concatenate the leveraged PLS factors into a single DataFrame\n",
    "lev_r_pls = pd.concat(lev_pls_dfs)\n",
    "lev_r_pls.fillna(0, inplace=True)\n",
    "lev_r_pls_clean = lev_r_pls.drop(lev_r_pls.index[:1])\n",
    "\n",
    "print(lev_r_pls_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2680b-ed86-410c-84c5-cc3550b2b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two boolean dataframes: one for positive average returns and one for negative average returns\n",
    "positive_returns_PLS = pls_avg_df > 0\n",
    "negative_returns_PLS = pls_avg_df < 0\n",
    "\n",
    "# convert the boolean dataframes to integers and 0's, one for long positions and one for short positions\n",
    "long_portfolio_PLS= positive_returns_PLS.astype(int)\n",
    "short_portfolio_PLS = negative_returns_PLS.astype(int)\n",
    "\n",
    "# create the 5 subsets of PCs\n",
    "mom_1_10 = ['PLS1', 'PLS2', 'PLS3', 'PLS4', 'PLS5', 'PLS6', 'PLS7', 'PLS8', 'PLS9', 'PLS10']\n",
    "mom_11_20 = ['PLS11', 'PLS12', 'PLS13', 'PLS14', 'PLS15', 'PLS16', 'PLS17', 'PLS18', 'PLS19', 'PLS20']\n",
    "mom_21_30 = ['PLS21', 'PLS22', 'PLS23', 'PLS24', 'PLS25', 'PLS26', 'PLS27', 'PLS28', 'PLS29', 'PLS30']\n",
    "mom_31_40 = ['PLS31', 'PLS32', 'PLS33', 'PLS34', 'PLS35', 'PLS36', 'PLS37', 'PLS38', 'PLS39', 'PLS40']\n",
    "mom_41_47 = ['PLS41', 'PLS42', 'PLS43', 'PLS44', 'PLS45', 'PLS46', 'PLS47']\n",
    "\n",
    "\n",
    "# create a list of the subsets for our loop\n",
    "mom_list = [mom_1_10, mom_11_20, mom_21_30, mom_31_40, mom_41_47]\n",
    "\n",
    "# create an empty dictionary \n",
    "r_mean_set_dict = {}\n",
    "\n",
    "# create a loop where the dummy dataframe is multiplied with the leveraged PC return dataframe. We shift the portfolio indicator with one, as we need to calculate the returns of t+1. \n",
    "for i, mom in enumerate(mom_list):\n",
    "    # create the strategy: the return of the long positions minus the return of the short positions (accounting for the fact that negative short returns need to become positive\n",
    "    r_PLS_set_mom = (long_portfolio_PLS[mom].shift(1) * lev_r_pls_clean[mom]) - (short_portfolio_PLS[mom].shift(1) * lev_r_pls_clean[mom])\n",
    "    # we take the mean of the returns of the 10 PC subsets\n",
    "    r_PLS_set_mean = r_PLS_set_mom.mean(axis=1)\n",
    "    # we append it to our dictionary\n",
    "    r_mean_set_dict[f'pls_set_{i + 1}'] = r_PLS_set_mean\n",
    "\n",
    "# create the dataframe with the series of returns for each subset of PCS\n",
    "mom_strategy3 = pd.concat(r_mean_set_dict, axis=1)\n",
    "\n",
    "mom_strategy3.index = pd.to_datetime(mom_strategy3.index)\n",
    "mom_strategy3.index = mom_strategy3.index.strftime('%Y-%m')\n",
    "mom_strategy3.dropna(inplace=True)\n",
    "\n",
    "print(mom_strategy3.mean(axis=0))\n",
    "print(mom_strategy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0c59d-adfa-49a3-a5fa-51594489eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the full sample dataframe in the paper and create the two splitted periods \n",
    "mom_strategy_full1 = mom_strategy3.loc['1973-07':'2019-12']\n",
    "mom_strategy_4 = mom_strategy3.loc['1973-07':'1996-09']\n",
    "mom_strategy_5 = mom_strategy3.loc['1996-09':]\n",
    "\n",
    "print(f'the mean of every subset of PCs is:\\n')\n",
    "print(mom_strategy_full1.mean(axis=0))\n",
    "\n",
    "means = mom_strategy3.mean(axis=0).tolist()\n",
    "std = mom_strategy3.std(axis=0).tolist()\n",
    "N = mom_strategy3.shape[0]\n",
    "\n",
    "print(f'\\n')\n",
    "print(f'the t-statistic of every subset of PCS is:\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)\n",
    "\n",
    "print(f'\\n')\n",
    "print(f'the mean of every subset of PCs is (first half):\\n')\n",
    "print(mom_strategy_4.mean(axis=0))\n",
    "\n",
    "means = mom_strategy_4.mean(axis=0).tolist()\n",
    "std = mom_strategy_4.std(axis=0).tolist()\n",
    "N = mom_strategy_4.shape[0]\n",
    "\n",
    "print(f'\\n')\n",
    "print(f'the t-statistic of every subset of PCS is (first half):\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)\n",
    "\n",
    "print(f'\\n')\n",
    "print(f'the mean of every subset of PCs is (second half):\\n')\n",
    "print(mom_strategy_5.mean(axis=0))\n",
    "\n",
    "means = mom_strategy_5.mean(axis=0).tolist()\n",
    "std = mom_strategy_5.std(axis=0).tolist()\n",
    "N = mom_strategy_5.shape[0]\n",
    "\n",
    "print(f'\\n')\n",
    "print(f'the t-statistic of every subset of PCS is (second half):\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f86e0c-bff5-414d-9bb7-1187f0fb6ce0",
   "metadata": {},
   "source": [
    "### Replicating panel B and C PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bc033-b40e-4fe9-bdea-62d7b1a4352f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/fffactors.dta'\n",
    "\n",
    "ff = pd.read_stata(url)\n",
    "\n",
    "# set index to date column\n",
    "ff.set_index('yyyymm', inplace=True)\n",
    "\n",
    "# set it to datetime format and correct format\n",
    "ff.index = pd.to_datetime(ff.index, format='%Y%m')\n",
    "ff.index = ff.index.strftime('%Y-%m')\n",
    "\n",
    "# select the needed factors over our sample \n",
    "ff5 = ff[['mktrf', 'smb', 'hml', 'rmw', 'cma']].loc['1973-07':'2019-12']\n",
    "\n",
    "# merge the dataframes together\n",
    "mom_strategy_ff5_pls = pd.concat([mom_strategy_full1, ff5], axis=1)\n",
    "\n",
    "# construct a dummy variable for period 1 and for period 2 \n",
    "mom_strategy_ff5_pls['P1'] = 0\n",
    "mom_strategy_ff5_pls['P2'] = 0\n",
    "mom_strategy_ff5_pls.loc[mom_strategy_ff5_pls.index <= '1996-09', 'P1'] = 1\n",
    "mom_strategy_ff5_pls.loc[mom_strategy_ff5_pls.index >= '1996-09', 'P2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca79c58-ba29-4162-84a9-9ba1588e685e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mom_strategy_ff5_pls.columns)\n",
    "print(mom_strategy_ff5_pls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0e09a-f2a3-4b6d-8695-5a921524124f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selecting our independent variables from our dataframe\n",
    "independent_vars = ['pls_set_1', 'P1', 'P2', 'mktrf', 'smb', 'hml', 'rmw', 'cma']\n",
    "X = mom_strategy_ff5_pls[independent_vars]\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "# constructing a loop that iterates the regressions\n",
    "for i in range(2, 6):\n",
    "    Y = pd.to_numeric(mom_strategy_ff5_pls[f'pls_set_{i}'], errors='coerce')\n",
    "    modelC = sm.OLS(Y, X, hasconst=False).fit()\n",
    "    print(modelC.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a41b2-5d3e-44fd-a000-39d4e00d54d8",
   "metadata": {},
   "source": [
    "### Panel C PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e868c7-03e6-4b3d-acfd-ee8a2a5272b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selecting our dependent variable\n",
    "y = pd.to_numeric(mom_strategy_ff5_pls[f'pls_set_1'], errors='coerce')\n",
    "# constructing a loop that iterates the regressions\n",
    "for i in range(2, 6):\n",
    "    independent_vars = [f'pls_set_{i}', 'P1', 'P2', 'mktrf', 'smb', 'hml', 'rmw', 'cma']\n",
    "    X = mom_strategy_ff5_pls[independent_vars]\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    modelD = sm.OLS(y, X, hasconst=False).fit()\n",
    "    print(modelD.summary())\n",
    "\n",
    "# last regression is one with all independent variables (panel C regression 5)\n",
    "independent_vars = ['P1', 'P2', 'pls_set_2', 'pls_set_3', 'pls_set_4', 'pls_set_5', 'mktrf', 'smb', 'hml', 'rmw', 'cma']\n",
    "X = mom_strategy_ff5_pls[independent_vars]\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "modelD1 = sm.OLS(y, X, hasconst=False).fit()\n",
    "print(modelD1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee16e2f5f76ebc",
   "metadata": {},
   "source": [
    "# Table 4 with PLS added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71178b7762679cb1",
   "metadata": {},
   "source": [
    "### loading data and merging dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01dc087ea1a2fd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:30:35.180009Z",
     "start_time": "2024-02-26T18:30:02.772645Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "merged_data = []\n",
    "\n",
    "# set url for the datafiles\n",
    "\n",
    "url_P10umd = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/P10UMD.dta'\n",
    "url_tsfactor = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/TSFactor.dta'\n",
    "url_umd = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/FactorUMD.dta'\n",
    "\n",
    "# import the datafiles and the needed columns\n",
    "p10umd = pd.read_stata(url_P10umd)\n",
    "tsfactor = pd.read_stata(url_tsfactor)[['year', 'month', 'TSMom', 'yyyymm']]\n",
    "factor_umd = pd.read_stata(url_umd)[['year', 'month', 'umd']]\n",
    "#oos_tsmom = pd.read_stata(\"oos_tsmom_scs.dta\")[['yyyymm', 'tsmom1', 'tsmom2', 'tsmom3', 'tsmom4', 'tsmom5']]\n",
    "\n",
    "# create yyyy-mm date indexes for tsfactor, p10umd and factor_umd\n",
    "\n",
    "tsfactor.set_index('yyyymm', inplace=True)\n",
    "tsfactor.index = pd.to_datetime(tsfactor.index, format='%Y%m')\n",
    "tsfactor.index = tsfactor.index.strftime('%Y-%m')\n",
    "tsfactor.drop(columns =['year', 'month'], inplace=True)\n",
    "\n",
    "p10umd['date'] = pd.to_datetime(p10umd['year'].astype(str) + '-' + p10umd['month'].astype(str))\n",
    "p10umd.set_index('date', inplace=True)\n",
    "p10umd.index = pd.to_datetime(p10umd.index, format='%Y%m')\n",
    "p10umd.index = p10umd.index.strftime('%Y-%m')\n",
    "p10umd.drop(columns =['year', 'month'], inplace=True)\n",
    "\n",
    "factor_umd['date'] = pd.to_datetime(factor_umd['year'].astype(str) + '-' + factor_umd['month'].astype(str))\n",
    "factor_umd.set_index('date', inplace=True)\n",
    "factor_umd.index = pd.to_datetime(factor_umd.index, format='%Y%m')\n",
    "factor_umd.index = factor_umd.index.strftime('%Y-%m')\n",
    "factor_umd.drop(columns =['year', 'month'], inplace=True)\n",
    "\n",
    "# select our appropriate time frame for table 4: July 1964 until December 2019 (except for our PC column, which has the data range from July 1973 until December 2019\n",
    "\n",
    "p10umd_range = p10umd.loc['1964-07':'2019-12']\n",
    "factor_umd_range = factor_umd.loc['1964-07':'2019-12']\n",
    "tsfactor_range = tsfactor.loc['1964-07':'2019-12']\n",
    "\n",
    "# we already loaded the ff5 in the previous cell, therefore we only adjust the time range to match table 4. \n",
    "ff5_range = ff[['rf', 'mktrf', 'smb', 'hml', 'rmw', 'cma']].loc['1964-07':'2019-12']\n",
    "\n",
    "# we multiply by 100, because our other datasets are in percentage\n",
    "ff5_range = ff5_range * 100\n",
    "\n",
    "# merge all datasets except for principal component return\n",
    "merged_data = pd.concat([p10umd_range, factor_umd_range, tsfactor_range, ff5_range], axis=1)\n",
    "print(merged_data)\n",
    "\n",
    "\n",
    "# multiply the columns of mom_strategy_full by 100\n",
    "mom_strategy_decile = mom_strategy_full * 100\n",
    "mom_strategy_decile1 = mom_strategy_full1 * 100\n",
    "\n",
    "# calculate the excess returns for the decile portfolios\n",
    "for i in range(1, 11):\n",
    "    merged_data[f'ExcessP{i}'] = merged_data[f'p{i}'] - merged_data['rf']\n",
    "\n",
    "# assign portfolio 11 for high-minus-low\n",
    "merged_data['ExcessP11'] = merged_data['ExcessP10'] - merged_data['ExcessP1']\n",
    "\n",
    "# Create a second dataframe for the Principal Component model, which has less observations (July 1973 unitl December 2019). Hence we first select all observations within this timeframe from our merged data set\n",
    "merged_data_partially = merged_data.loc['1973-07':'2019-12']\n",
    "merged_data_pca = pd.concat([merged_data_partially, mom_strategy_decile], axis=1)\n",
    "\n",
    "# Create a second dataframe for the PLS model, which has less observations (July 1973 unitl December 2019). Hence we first select all observations within this timeframe from our merged data set\n",
    "merged_data_pls = pd.concat([merged_data_pca, mom_strategy_decile1], axis=1)\n",
    "\n",
    "print(merged_data_pls.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67b23b69ca6ac5",
   "metadata": {},
   "source": [
    "### replicating panel A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d3bc92f6485d8",
   "metadata": {},
   "source": [
    "### the fama french 5 factor model (ff5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cd4569b3ca321",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an empty lists for the estimate results of the fama french 5 factor model \n",
    "estimatesFF5_list = []\n",
    "\n",
    "# momentum sorted portfolios with FF5\n",
    "for i in range(1, 12):\n",
    "    X = sm.add_constant(merged_data[['mktrf', 'smb', 'hml', 'cma', 'rmw']])\n",
    "    model = sm.OLS(merged_data[f'ExcessP{i}'], X).fit()\n",
    "    estimatesFF5_list.append(model)\n",
    "\n",
    "#for i, est in enumerate(estimatesFF5_list, start=1):\n",
    "    #print(f\"\\nRegression Results for Excess Portfolio Returns with FF5 - Portfolio {i}:\\n\")\n",
    "    #print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe5d138500c644",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Results for FF5 Model:\\n\")\n",
    "resultsFF5_list = []\n",
    "\n",
    "# calculating the results for the Fama French 5 factor model \n",
    "for i, est_FF5 in enumerate(estimatesFF5_list, start=1):\n",
    "    result_row = {\n",
    "        'Decile': f'{i}',\n",
    "        'FF5_Alpha': f'{est_FF5.params[\"const\"]:.2f}',\n",
    "    }\n",
    "    t_stat_row = {\n",
    "        'Decile': f'',\n",
    "        'FF5_Alpha': f'({est_FF5.tvalues[\"const\"]:.2f})',\n",
    "    }\n",
    "    resultsFF5_list.append(result_row)\n",
    "    resultsFF5_list.append(t_stat_row)\n",
    "\n",
    "resultsFF5_df = pd.DataFrame(resultsFF5_list)\n",
    "print(resultsFF5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3245cf017d1aa3a",
   "metadata": {},
   "source": [
    "### the umd model (including ff5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0e429981781ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "estimatesUMD_list = []\n",
    "\n",
    "# Momentum sorted portfolios with FF5 + UMD\n",
    "for i in range(1, 12):\n",
    "    X = sm.add_constant(merged_data[['mktrf', 'smb', 'hml', 'cma', 'rmw', 'umd']])\n",
    "    model = sm.OLS(merged_data[f'ExcessP{i}'], X).fit()\n",
    "    estimatesUMD_list.append(model)\n",
    "\n",
    "#for i, est in enumerate(estimatesUMD_list, start=1):\n",
    "    #print(f\"\\nRegression Results for Excess Portfolio Returns with FF5 + UMD - Portfolio {i}:\\n\")\n",
    "    #print(est.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43814d6442c1c2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Results for FF5 + UMD Model:\\n\")\n",
    "resultsUMD_list = []\n",
    "\n",
    "# Results for FF5 + UMD\n",
    "for i, est_umd in enumerate(estimatesUMD_list, start=1):\n",
    "    result_row = {\n",
    "        'Decile': f'{i}',\n",
    "        'UMD_Alpha': f'{est_umd.params[\"const\"]:.2f}',\n",
    "        'UMD_Coefficient': f'{est_umd.params[\"umd\"]:.2f}'\n",
    "    }\n",
    "    t_stat_row = {\n",
    "        'Decile': f'',\n",
    "        'UMD_Alpha': f'({est_umd.tvalues[\"const\"]:.2f})',\n",
    "        'UMD_Coefficient': f'({est_umd.tvalues[\"umd\"]:.2f})'\n",
    "    }\n",
    "    resultsUMD_list.append(result_row)\n",
    "    resultsUMD_list.append(t_stat_row)\n",
    "\n",
    "resultsUMD_df = pd.DataFrame(resultsUMD_list)\n",
    "print(resultsUMD_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb317ad17a9ac90",
   "metadata": {},
   "source": [
    "### factor momentum (using factors from table 1) including ff5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce7f535f24d44b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "estimatesFMOMind_list = []\n",
    "\n",
    "# Momentum sorted portfolios with FF5 + MOMind\n",
    "for i in range(1, 12):\n",
    "    X = sm.add_constant(merged_data[['mktrf', 'smb', 'hml', 'cma', 'rmw', 'TSMom']])\n",
    "    model = sm.OLS(merged_data[f'ExcessP{i}'], X).fit()\n",
    "    estimatesFMOMind_list.append(model)\n",
    "\n",
    "#for i, est in enumerate(estimatesFMOMind_list, start=1):\n",
    "    #print(f\"\\nRegression Results for Excess Portfolio Returns with FF5 + FMOMind - Portfolio {i}:\\n\")\n",
    "    #print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1132e192a1d62fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Results for FF5 + FMOMind Model:\\n\")\n",
    "resultsFMOMind_list = []\n",
    "\n",
    "# Results for FF5 + FMOMind\n",
    "for i, est_FMOMind in enumerate(estimatesFMOMind_list, start=1):\n",
    "    result_row = {\n",
    "        'Decile': f'{i}',\n",
    "        'FMOMind_Alpha': f'{est_FMOMind.params[\"const\"]:.2f}',\n",
    "        'FMOMind_Coefficient': f'{est_FMOMind.params[\"TSMom\"]:.2f}'\n",
    "    }\n",
    "    t_stat_row = {\n",
    "        'Decile': f'',\n",
    "        'FMOMind_Alpha': f'({est_FMOMind.tvalues[\"const\"]:.2f})',\n",
    "        'FMOMind_Coefficient': f'({est_FMOMind.tvalues[\"TSMom\"]:.2f})'\n",
    "    }\n",
    "    resultsFMOMind_list.append(result_row)\n",
    "    resultsFMOMind_list.append(t_stat_row)\n",
    "\n",
    "resultsFMOMind_df = pd.DataFrame(resultsFMOMind_list)\n",
    "print(resultsFMOMind_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0eb3f8782e5a51",
   "metadata": {},
   "source": [
    "### factor momentum (using the PC factors 1-10) including ff5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44446a9ba4b1e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "estimatesFMOMpc_list = []\n",
    "\n",
    "# Momentum sorted portfolios with FF5 + UMD\n",
    "for i in range(1, 12):\n",
    "    X = sm.add_constant(merged_data_pca[['mktrf', 'smb', 'hml', 'cma', 'rmw', 'mom_set_1']])\n",
    "    model = sm.OLS(merged_data_pca[f'ExcessP{i}'], X).fit()\n",
    "    estimatesFMOMpc_list.append(model)\n",
    "\n",
    "#for i, est in enumerate(estimatesFMOMpc_list, start=1):\n",
    "    #print(f\"\\nRegression Results for Excess Portfolio Returns with FF5 + FMOMpc - Portfolio {i}:\\n\")\n",
    "    #print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44737d8253f4e525",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Results for FF5 + FMOMpc Model:\\n\")\n",
    "resultsFMOMpc_list = []\n",
    "\n",
    "# Append results for FF5 + FMOMpc\n",
    "for i, est_FMOMpc in enumerate(estimatesFMOMpc_list, start=1):\n",
    "    result_row = {\n",
    "        'Decile': f'{i}',\n",
    "        'FMOMpc_Alpha': f'{est_FMOMpc.params[\"const\"]:.2f}',\n",
    "        'FMOMpc_Coefficient': f'{est_FMOMpc.params[\"mom_set_1\"]:.2f}'\n",
    "    }\n",
    "    t_stat_row = {\n",
    "        'Decile': f'',\n",
    "        'FMOMpc_Alpha': f'({est_FMOMpc.tvalues[\"const\"]:.2f})',\n",
    "        'FMOMpc_Coefficient': f'({est_FMOMpc.tvalues[\"mom_set_1\"]:.2f})'\n",
    "    }\n",
    "    resultsFMOMpc_list.append(result_row)\n",
    "    resultsFMOMpc_list.append(t_stat_row)\n",
    "\n",
    "# convert the list of results to a DataFrame\n",
    "resultsFMOMpc_df = pd.DataFrame(resultsFMOMpc_list)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(resultsFMOMpc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd9ab5-6e95-4b10-8e00-72a21ce17444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_inf_nan = merged_data_pls.isin([np.inf, -np.inf, np.nan]).sum().sum()\n",
    "\n",
    "print(f\"Count of inf and nan values: {count_inf_nan}\")\n",
    "\n",
    "merged_data_pls.replace([np.inf, -np.inf, np.nan], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0a030-2dd6-42ab-b936-1b61cd5fa0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimatesFMOMpls_list = []\n",
    "\n",
    "# Momentum sorted portfolios with FF5 + PLS\n",
    "for i in range(1, 12):\n",
    "    X = sm.add_constant(merged_data_pls[['mktrf', 'smb', 'hml', 'cma', 'rmw', 'pls_set_1']])\n",
    "    model = sm.OLS(merged_data_pls[f'ExcessP{i}'], X).fit()\n",
    "    estimatesFMOMpls_list.append(model)\n",
    "\n",
    "#for i, est in enumerate(estimatesFMOMpc_list, start=1):\n",
    "    #print(f\"\\nRegression Results for Excess Portfolio Returns with FF5 + FMOMpc - Portfolio {i}:\\n\")\n",
    "    #print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99780d6-8946-45ad-8677-2c75d822e19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Results for FF5 + FMOMpls Model:\\n\")\n",
    "resultsFMOMpls_list = []\n",
    "\n",
    "# Append results for FF5 + FMOMpls\n",
    "for i, est_FMOMpls in enumerate(estimatesFMOMpls_list, start=1):\n",
    "    result_row = {\n",
    "        'Decile': f'{i}',\n",
    "        'FMOMpls_Alpha': f'{est_FMOMpls.params[\"const\"]:.2f}',\n",
    "        'FMOMpls_Coefficient': f'{est_FMOMpls.params[\"pls_set_1\"]:.2f}'\n",
    "    }\n",
    "    t_stat_row = {\n",
    "        'Decile': f'',\n",
    "        'FMOMpls_Alpha': f'({est_FMOMpls.tvalues[\"const\"]:.2f})',\n",
    "        'FMOMpls_Coefficient': f'({est_FMOMpls.tvalues[\"pls_set_1\"]:.2f})'\n",
    "    }\n",
    "    resultsFMOMpls_list.append(result_row)\n",
    "    resultsFMOMpls_list.append(t_stat_row)\n",
    "\n",
    "# convert the list of results to a DataFrame\n",
    "resultsFMOMpls_df = pd.DataFrame(resultsFMOMpls_list)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(resultsFMOMpls_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cfbdcdcfe055da",
   "metadata": {},
   "source": [
    "### merging results into onde dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e348150b8e29",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "resultsUMD_df = resultsUMD_df.drop(columns=['Decile'])\n",
    "resultsFMOMind_df = resultsFMOMind_df.drop(columns=['Decile'])\n",
    "resultsFMOMpc_df = resultsFMOMpc_df.drop(columns=['Decile'])\n",
    "resultsFMOMpls_df = resultsFMOMpls_df.drop(columns=['Decile'])\n",
    "\n",
    "# Merge the results\n",
    "table_results = pd.concat([resultsFF5_df, resultsUMD_df, resultsFMOMind_df, resultsFMOMpc_df, resultsFMOMpls_df], axis=1)\n",
    "\n",
    "print(table_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10073f00cc9615a2",
   "metadata": {},
   "source": [
    "### calculating alphas for the respective models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f995efa56d436",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_means_FF5 = []\n",
    "alpha_means_UMD = []\n",
    "alpha_means_FMOMind = []\n",
    "alpha_means_FMOMpc = []\n",
    "alpha_means_FMOMpls = []\n",
    "\n",
    "# calculate the absolute values of the alphas\n",
    "for i, est_ff5 in enumerate(estimatesFF5_list, start=1):\n",
    "    alpha_mean_ff5 = np.abs(est_ff5.params['const']).mean()\n",
    "    alpha_means_FF5.append(alpha_mean_ff5)\n",
    "\n",
    "for i, est_umd in enumerate(estimatesUMD_list, start=1):\n",
    "    alpha_mean_umd = np.abs(est_umd.params['const']).mean()\n",
    "    alpha_means_UMD.append(alpha_mean_umd)\n",
    "\n",
    "for i, est_fmomind in enumerate(estimatesFMOMind_list, start=1):\n",
    "    alpha_mean_fmomind = np.abs(est_fmomind.params['const']).mean()\n",
    "    alpha_means_FMOMind.append(alpha_mean_fmomind)\n",
    "\n",
    "for i, est_fmompc in enumerate(estimatesFMOMpc_list, start=1):\n",
    "    alpha_mean_fmompc = np.abs(est_fmompc.params['const']).mean()\n",
    "    alpha_means_FMOMpc.append(alpha_mean_fmompc)\n",
    "    \n",
    "for i, est_fmompls in enumerate(estimatesFMOMpls_list, start=1):\n",
    "    alpha_mean_fmompls = np.abs(est_fmompls.params['const']).mean()\n",
    "    alpha_means_FMOMpls.append(alpha_mean_fmompls)\n",
    "\n",
    "# calculate the absolute mean alphas for the different models except for the winners-losers portfolio\n",
    "Avg_alpha_FF5 = np.mean(alpha_means_FF5[:-1])\n",
    "Avg_alpha_UMD = np.mean(alpha_means_UMD[:-1])\n",
    "Avg_alpha_FMOMind = np.mean(alpha_means_FMOMind[:-1])\n",
    "Avg_alpha_FMOMpc = np.mean(alpha_means_FMOMpc[:-1])\n",
    "Avg_alpha_FMOMpls = np.mean(alpha_means_FMOMpls[:-1])\n",
    "\n",
    "# create a DataFrame for average alphas\n",
    "avg_alphas_df = pd.DataFrame({\n",
    "    'Model': ['FF5', 'UMD', 'FMOMind', 'FMOMpc', 'FMOMpls'],\n",
    "    'Avg_alpha': [Avg_alpha_FF5, Avg_alpha_UMD, Avg_alpha_FMOMind, Avg_alpha_FMOMpc, Avg_alpha_FMOMpls]\n",
    "})\n",
    "\n",
    "# print the results\n",
    "print(avg_alphas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e10fb5627513ab",
   "metadata": {},
   "source": [
    "### Replicating panel B PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f08406e48435fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selecting our independent variables (using umd as our dependent variable)\n",
    "independent_vars_ff5 = ['mktrf', 'smb', 'hml', 'cma', 'rmw']\n",
    "\n",
    "# Augment the FF5 model with different subsets of PC factors\n",
    "independent_vars_mom1 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'mom_set_1']\n",
    "independent_vars_mom2 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'mom_set_2']\n",
    "independent_vars_mom3 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'mom_set_3']\n",
    "independent_vars_mom4 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'mom_set_4']\n",
    "independent_vars_mom5 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'mom_set_5']\n",
    "\n",
    "# create a list to store the results\n",
    "results_list = []\n",
    "\n",
    "# fit the FF5 model\n",
    "X_ff5 = sm.add_constant(merged_data_pca[independent_vars_ff5])\n",
    "model_ff5 = sm.OLS(merged_data_pca['umd'], X_ff5)\n",
    "results_ff5 = model_ff5.fit()\n",
    "\n",
    "# record the results for the FF5 model (only alpha)\n",
    "results_list.append({\n",
    "    'Model': 'FF5',\n",
    "    'Alpha': results_ff5.params['const'],\n",
    "    'Alpha T-stat': results_ff5.tvalues['const'],\n",
    "    'FMom Slope':' ',\n",
    "    'FMom Slope T-stat':' ',\n",
    "    'R-squared_adj': results_ff5.rsquared_adj\n",
    "})\n",
    "\n",
    "# Fit models with different subsets of PC factors\n",
    "for subset_vars in [independent_vars_mom1, independent_vars_mom2, independent_vars_mom3, independent_vars_mom4, independent_vars_mom5]:\n",
    "    X_subset = sm.add_constant(merged_data_pca[subset_vars])\n",
    "    model_subset = sm.OLS(merged_data_pca['umd'], X_subset)\n",
    "    results_subset = model_subset.fit()\n",
    "\n",
    "    # Record the results for each subset, including the alpha and slope for pctsmom factor\n",
    "    results_list.append({\n",
    "        'Model': ' + '.join(subset_vars[-1:]),\n",
    "        'Alpha': results_subset.params['const'],\n",
    "        'Alpha T-stat': results_subset.tvalues['const'],\n",
    "        'FMom Slope': results_subset.params[subset_vars[-1:][0]],\n",
    "        'FMom Slope T-stat': results_subset.tvalues[subset_vars[-1:][0]],\n",
    "        'R-squared_adj': results_subset.rsquared_adj\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e0cb8-4659-4bc9-94cc-b86160df40dd",
   "metadata": {},
   "source": [
    "### Replicating Panel B PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ee625-5926-4e5b-a359-125a727aebe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selecting our independent variables (using umd as our dependent variable)\n",
    "independent_vars_ff5 = ['mktrf', 'smb', 'hml', 'cma', 'rmw']\n",
    "\n",
    "# Augment the FF5 model with different subsets of PC factors\n",
    "independent_vars_mom1 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'pls_set_1']\n",
    "independent_vars_mom2 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'pls_set_2']\n",
    "independent_vars_mom3 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'pls_set_3']\n",
    "independent_vars_mom4 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'pls_set_4']\n",
    "independent_vars_mom5 = ['mktrf', 'smb', 'hml', 'cma', 'rmw', 'pls_set_5']\n",
    "\n",
    "# create a list to store the results\n",
    "results_list = []\n",
    "\n",
    "# fit the FF5 model\n",
    "X_ff5 = sm.add_constant(merged_data_pls[independent_vars_ff5])\n",
    "model_ff5 = sm.OLS(merged_data_pls['umd'], X_ff5)\n",
    "results_ff5 = model_ff5.fit()\n",
    "\n",
    "# record the results for the FF5 model (only alpha)\n",
    "results_list.append({\n",
    "    'Model': 'FF5',\n",
    "    'Alpha': results_ff5.params['const'],\n",
    "    'Alpha T-stat': results_ff5.tvalues['const'],\n",
    "    'FMom Slope':' ',\n",
    "    'FMom Slope T-stat':' ',\n",
    "    'R-squared_adj': results_ff5.rsquared_adj\n",
    "})\n",
    "\n",
    "# Fit models with different subsets of PC factors\n",
    "for subset_vars in [independent_vars_mom1, independent_vars_mom2, independent_vars_mom3, independent_vars_mom4, independent_vars_mom5]:\n",
    "    X_subset = sm.add_constant(merged_data_pls[subset_vars])\n",
    "    model_subset = sm.OLS(merged_data_pls['umd'], X_subset)\n",
    "    results_subset = model_subset.fit()\n",
    "\n",
    "    # Record the results for each subset, including the alpha and slope for pctsmom factor\n",
    "    results_list.append({\n",
    "        'Model': ' + '.join(subset_vars[-1:]),\n",
    "        'Alpha': results_subset.params['const'],\n",
    "        'Alpha T-stat': results_subset.tvalues['const'],\n",
    "        'FMom Slope': results_subset.params[subset_vars[-1:][0]],\n",
    "        'FMom Slope T-stat': results_subset.tvalues[subset_vars[-1:][0]],\n",
    "        'R-squared_adj': results_subset.rsquared_adj\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75921c4-3d07-4386-8798-99978befe799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
