{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3b02ed9dd222dd"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T14:59:00.870501Z",
     "start_time": "2024-02-26T14:59:00.868953Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import calendar\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table 1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ed0fca218438e6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7d7850c72b3792a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac' 'bab' 'cfp' 'cma' 'ep' 'hml' 'liq' 'ltrev' 'nsi' 'qmj' 'rmw' 'rvar'\n",
      " 'smb' 'strev' 'umd' 'glbab' 'glcma' 'glhml' 'glqmj' 'glrmw' 'glsmb'\n",
      " 'glumd']\n",
      "       year  month anomaly    ret   time  global\n",
      "0      1963      7      ac  2.170   42.0     0.0\n",
      "1      1963      8      ac -0.197   43.0     0.0\n",
      "2      1963      9      ac  0.600   44.0     0.0\n",
      "3      1963     10      ac  6.463   45.0     0.0\n",
      "4      1963     11      ac -2.260   46.0     0.0\n",
      "...     ...    ...     ...    ...    ...     ...\n",
      "10111  2019      8     umd  7.600  715.0     0.0\n",
      "10112  2019      9     umd -6.850  716.0     0.0\n",
      "10113  2019     10     umd  0.240  717.0     0.0\n",
      "10114  2019     11     umd -2.620  718.0     0.0\n",
      "10115  2019     12     umd -2.130  719.0     0.0\n",
      "\n",
      "[10116 rows x 6 columns]\n",
      "       year  month anomaly       ret   time  global\n",
      "10116  1987      2   glbab  2.236918  325.0     1.0\n",
      "10117  1987      3   glbab  1.828450  326.0     1.0\n",
      "10118  1987      4   glbab -5.521739  327.0     1.0\n",
      "10119  1987      5   glbab -0.513814  328.0     1.0\n",
      "10120  1987      6   glbab  1.579217  329.0     1.0\n",
      "...     ...    ...     ...       ...    ...     ...\n",
      "12638  2019      8   glumd  2.990000  715.0     1.0\n",
      "12639  2019      9   glumd -3.260000  716.0     1.0\n",
      "12640  2019     10   glumd -0.940000  717.0     1.0\n",
      "12641  2019     11   glumd  0.000000  718.0     1.0\n",
      "12642  2019     12   glumd  0.740000  719.0     1.0\n",
      "\n",
      "[2527 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/anomalies.dta'\n",
    "\n",
    "Anomalies = pd.read_stata(url)\n",
    "\n",
    "# display unique values in the 'anomaly' column\n",
    "print(Anomalies['anomaly'].unique())\n",
    "\n",
    "# delete the global factors from the dataframe and create Anomalies US\n",
    "column_name = 'anomaly'\n",
    "values_to_dropUS = ['glbab', 'glcma', 'glhml', 'glqmj', 'glrmw', 'glsmb', 'glumd']\n",
    "ElementsUS = Anomalies[column_name].isin(values_to_dropUS)\n",
    "Anomalies_US = Anomalies[~ElementsUS]\n",
    "\n",
    "# delete the US factors from dataframe and create Anomalies Global \n",
    "column_name = 'anomaly'\n",
    "values_to_dropGF = ['ac', 'bab', 'cfp', 'cma', 'ep', 'hml', 'liq', 'ltrev', 'nsi', 'qmj', 'rmw', 'rvar',\n",
    "                    'smb', 'strev', 'umd']\n",
    "ElementsGF = Anomalies[column_name].isin(values_to_dropGF)\n",
    "Anomalies_GF = Anomalies[~ElementsGF]\n",
    "\n",
    "# print both anomalies US and anomalies global \n",
    "print(Anomalies_US)\n",
    "print(Anomalies_GF)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T13:12:17.659382Z",
     "start_time": "2024-02-26T13:12:16.740681Z"
    }
   },
   "id": "aefa9a33ec9759a5",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### replicating table 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f2b93f47c09c62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### US factors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3b4c916e084c319"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   anomaly  Mean     SD T-value\n",
      "0       ac  2.8%   6.6%    3.19\n",
      "1      bab  9.8%  11.2%    6.55\n",
      "2      cfp  3.4%   8.6%    2.94\n",
      "3      cma  3.3%   6.9%    3.59\n",
      "4       ep  3.5%   8.9%    2.95\n",
      "5      hml  3.6%   9.7%    2.82\n",
      "6      liq  4.4%  11.6%    2.77\n",
      "7    ltrev  2.5%   8.7%    2.16\n",
      "8      nsi  2.8%   8.2%    2.52\n",
      "9      qmj  4.6%   7.7%    4.47\n",
      "10     rmw  3.1%   7.5%    3.13\n",
      "11    rvar  1.6%  17.3%    0.68\n",
      "12     smb  2.7%  10.4%    1.97\n",
      "13   strev  6.0%  10.6%    4.21\n",
      "14     umd  7.8%  14.5%    4.02\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean and stand deviation of US factors\n",
    "AnomaliesUS = Anomalies_US.groupby(['anomaly']).agg({'ret': ['mean', 'std', 'count']}).reset_index()\n",
    "AnomaliesUS = Anomalies_US.pivot_table(index='anomaly', values='ret', aggfunc=['mean', 'std', 'count'])\n",
    "AnomaliesUS.columns = ['Mean', 'SD', 'ret_number']\n",
    "AnomaliesUS.reset_index(inplace=True)\n",
    "AnomaliesUS.columns = ['anomaly', 'Mean', 'SD', 'ret_number']\n",
    "\n",
    "# calculate additional statistics\n",
    "AnomaliesUS['ret_semean'] = AnomaliesUS['SD'] / np.sqrt(AnomaliesUS['ret_number'])\n",
    "\n",
    "# multiply by 12 to create annualized returns\n",
    "AnomaliesUS['ret'] = AnomaliesUS['Mean'] * 12\n",
    "\n",
    "# multiply by sqrt(12) to create annualized standard deviation\n",
    "AnomaliesUS['sd'] = AnomaliesUS['SD'] * np.sqrt(12)\n",
    "\n",
    "# calculate the t-stat by dividing the return by the standard error of the mean. Divide by 12 to annualize it\n",
    "AnomaliesUS['tstat'] = AnomaliesUS['ret'] / AnomaliesUS['ret_semean'] /12\n",
    "\n",
    "# format the table to correct decimals\n",
    "AnomaliesUS[['Mean']] = AnomaliesUS[['ret']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesUS[['SD']] = AnomaliesUS[['sd']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesUS[['T-value']] = AnomaliesUS[['tstat']].apply(lambda x: x.map(\"{:.2f}\".format))\n",
    "\n",
    "print(AnomaliesUS[['anomaly','Mean','SD','T-value']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T13:12:32.874029Z",
     "start_time": "2024-02-26T13:12:32.830780Z"
    }
   },
   "id": "66347171583346c7",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Global factors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f970bbebcafdd90"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anomaly  Mean     SD T-value\n",
      "0   glbab  9.6%   9.7%    5.70\n",
      "1   glcma  1.9%   6.0%    1.74\n",
      "2   glhml  4.0%   7.4%    2.92\n",
      "3   glqmj  6.2%   6.8%    5.06\n",
      "4   glrmw  4.3%   4.7%    4.91\n",
      "5   glsmb  1.1%   7.1%    0.83\n",
      "6   glumd  7.9%  12.1%    3.54\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean and stand deviation of Global factors\n",
    "AnomaliesGF = Anomalies_GF.groupby(['anomaly']).agg({'ret': ['mean', 'std', 'count']}).reset_index()\n",
    "AnomaliesGF = Anomalies_GF.pivot_table(index='anomaly', values='ret', aggfunc=['mean', 'std', 'count'])\n",
    "AnomaliesGF.columns = ['Mean', 'SD', 'ret_number']\n",
    "AnomaliesGF.reset_index(inplace=True)\n",
    "AnomaliesGF.columns = ['anomaly', 'Mean', 'SD', 'ret_number']\n",
    "\n",
    "# calculate additional statistics\n",
    "AnomaliesGF['ret_semean'] = AnomaliesGF['SD'] / np.sqrt(AnomaliesGF['ret_number'])\n",
    "\n",
    "# multiply by 12 to create annualized returns\n",
    "AnomaliesGF['ret'] = AnomaliesGF['Mean'] * 12\n",
    "\n",
    "# multiply by sqrt(12) to create annualized standard deviation\n",
    "AnomaliesGF['sd'] = AnomaliesGF['SD'] * np.sqrt(12)\n",
    "\n",
    "# calculate the t-stat by dividing the return by the standard error of the mean. Divide by 12 to annualize it\n",
    "AnomaliesGF['tstat'] = AnomaliesGF['ret'] / AnomaliesGF['ret_semean'] /12\n",
    "\n",
    "# format the table to correct decimals\n",
    "AnomaliesGF[['Mean']] = AnomaliesGF[['ret']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesGF[['SD']] = AnomaliesGF[['sd']].apply(lambda x: x.map(\"{:.1f}%\".format))\n",
    "AnomaliesGF[['T-value']] = AnomaliesGF[['tstat']].apply(lambda x: x.map(\"{:.2f}\".format))\n",
    "\n",
    "print(AnomaliesGF[['anomaly','Mean','SD','T-value']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T13:12:42.065180Z",
     "start_time": "2024-02-26T13:12:42.044718Z"
    }
   },
   "id": "bbd214b9e39f7b82",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table 2 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a16892a0e1af284"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### replicating table 2 (uses the same dataset as table 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a5369eb282369a4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Anomaly     Alpha  T-stat_Alpha     Slope  T-stat_Slope\n",
      "0       ac  0.150195      1.184450  0.101410      0.649822\n",
      "1      bab -0.221412     -0.632211  1.319041      3.534152\n",
      "2      cfp  0.127745      0.781292  0.235454      1.157989\n",
      "3      cma  0.120082      0.974474  0.244693      1.545819\n",
      "4       ep  0.101357      0.616107  0.302075      1.458207\n",
      "5      hml  0.038477      0.204762  0.410255      1.780679\n",
      "6      liq  0.157215      0.741922  0.356063      1.291807\n",
      "7    ltrev -0.252989     -1.663307  0.757680      3.850110\n",
      "8      nsi  0.172982      1.324451  0.089249      0.486779\n",
      "9      qmj  0.086832      0.650364  0.434757      2.507550\n",
      "10     rmw  0.040360      0.222250  0.337185      1.673841\n",
      "11    rvar -0.463569     -1.638345  1.061609      2.737366\n",
      "12     smb -0.104191     -0.615583  0.583455      2.508982\n",
      "13   strev  0.485098      1.427336  0.013888      0.038600\n",
      "14     umd  0.716042      2.697340 -0.094969     -0.288098\n",
      "15   glbab  0.190820      0.577502  0.837610      2.303918\n",
      "16   glcma -0.064014     -0.408285  0.382064      1.944285\n",
      "17   glhml  0.035556      0.150752  0.471689      1.770057\n",
      "18   glqmj  0.394512      1.761234  0.124643      0.492025\n",
      "19   glrmw  0.137826      1.033410  0.256716      1.616411\n",
      "20   glsmb -0.063285     -0.388832  0.284797      1.325669\n",
      "21   glumd  0.668710      1.774142  0.017124      0.039403\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# create a loop which iterates over each anomly in our dataset \n",
    "for anomaly in Anomalies['anomaly'].unique():\n",
    "    subset = Anomalies[Anomalies['anomaly'] == anomaly]\n",
    "    subset = subset.sort_values(by='time')\n",
    "\n",
    "    # create a binary variable for positive returns in the past 12 months (the signal variable)\n",
    "    subset['positive_return'] = subset['ret'].rolling(window=12, min_periods=12).mean().shift(1) > 0\n",
    "\n",
    "    # drop the first 12 observations in the subset after the rolling window has been applied (so dropping N/A values)\n",
    "    subset = subset.iloc[12:]\n",
    "\n",
    "    # select our OLS model and fit our data\n",
    "    y = subset['ret']\n",
    "    X = sm.add_constant(subset['positive_return'].astype(int))\n",
    "    model = sm.OLS(y, X)\n",
    "    \n",
    "    # select the correct covariance type (as used in the paper)\n",
    "    results = model.fit(cov_type='cluster', cov_kwds={'groups': subset['time']})\n",
    "\n",
    "    # append the results to our dictionary to create the results\n",
    "    results_list.append({\n",
    "        'anomaly': anomaly,\n",
    "        'alpha': results.params['const'],\n",
    "        'T-stat_alpha': results.tvalues['const'],\n",
    "        'slope': results.params['positive_return'],\n",
    "        'T-stat_slope': results.tvalues['positive_return'],\n",
    "    })\n",
    "\n",
    "results_table = pd.DataFrame(results_list)\n",
    "print(results_table)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T13:22:38.886376Z",
     "start_time": "2024-02-26T13:22:38.813337Z"
    }
   },
   "id": "d64fbe7d50f9ec9",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5405d5744d09b948"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f19fe09105a82214"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# loading dataframes \n",
    "\n",
    "url = 'https://github.com/Sebasleen/Seminargroup/raw/Seminar/Data/managed_portfolios_anom_d_55.csv'\n",
    "\n",
    "r_daily = pd.read_csv(url)\n",
    "\n",
    "# drop all momentum factors or factors that are constructed based on momentum (including market return variables)\n",
    "\n",
    "factor_drop_list = ['r_mom', 'r_indmom', 'r_valmom', 'r_valmomprof', 'r_mom12', 'r_momrev', 'r_indmomrev', 'r_exchsw', 'rme', 're_ew']\n",
    "\n",
    "r_daily.drop(columns=factor_drop_list, inplace=True)\n",
    "\n",
    "# set date to datetime format and set the date to the index \n",
    "\n",
    "r_daily['date'] = pd.to_datetime(r_daily['date'])\n",
    "r_daily.set_index('date', inplace=True)\n",
    "\n",
    "# following the procedure in the paper, if there are observations missing we set them to 0. (footnote 16)\n",
    "\n",
    "r_daily.fillna(0, inplace=True)\n",
    "\n",
    "# create a list of factors for later analysis purposes \n",
    "\n",
    "factors = [col for col in r_daily.columns if col.startswith('r_')]\n",
    "\n",
    "# create a monthly return dataframe for later analysis purposes (by summing the daily returns)\n",
    "\n",
    "r_monthly = r_daily.resample('ME').sum()\n",
    "r_monthly.index = r_monthly.index.strftime('%Y-%m')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T13:43:12.562660Z",
     "start_time": "2024-02-26T13:43:06.532381Z"
    }
   },
   "id": "53b534a6439d1efb",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### perform the PCA analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e6714174c41bb33"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "date                                                                            \n",
      "1973-02 -0.163527  -0.10435 -0.004879  0.001906  0.080599   0.11734  0.004165   \n",
      "1973-03 -0.254274  -0.04289 -0.002538 -0.097974 -0.033213 -0.057495   0.00577   \n",
      "1973-04 -0.166303  0.053257  0.051769 -0.043656  0.022396  0.051989    0.0334   \n",
      "1973-05 -0.403385  0.076711   0.09642  -0.07831    0.0444  0.073014  0.079326   \n",
      "1973-06 -0.100178 -0.117113 -0.034034 -0.069168  0.000694  0.010336  0.061616   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "2019-09    0.0618  -0.26667   -0.0452  0.144064  0.059383 -0.041025  -0.05225   \n",
      "2019-10 -0.418177  0.059967  0.232239 -0.056775 -0.086553 -0.024549  0.072874   \n",
      "2019-11  0.035635 -0.109379  0.016161 -0.062541    0.0222  0.060263  0.078533   \n",
      "2019-12   0.20419  0.041358 -0.038417   0.01059 -0.027231  0.046026   0.01053   \n",
      "2020-01 -0.072692  0.050692  0.067176 -0.032406  0.022009 -0.003676  0.002061   \n",
      "\n",
      "              PC8       PC9      PC10  ...      PC38      PC39      PC40  \\\n",
      "date                                   ...                                 \n",
      "1973-02  0.003035 -0.079989 -0.020695  ... -0.021327  0.008742   -0.0051   \n",
      "1973-03  0.042633 -0.035161 -0.017398  ...  0.005376   -0.0003 -0.000367   \n",
      "1973-04  0.022461 -0.026694 -0.010816  ...  0.019176 -0.001799  0.003847   \n",
      "1973-05  0.073523 -0.055137  0.005008  ... -0.026462  0.000317 -0.007336   \n",
      "1973-06   0.12224  0.016014  0.004321  ... -0.006241 -0.000443  0.006889   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "2019-09   0.01717 -0.040238 -0.030897  ... -0.001199  0.008439 -0.021196   \n",
      "2019-10 -0.016393  0.047308  0.047244  ...  0.010715  0.004885  0.001281   \n",
      "2019-11  -0.04381  0.036236   0.03833  ...  0.005283 -0.005292  0.014402   \n",
      "2019-12  0.021675 -0.026845  0.012445  ...  0.001963 -0.005495  0.007557   \n",
      "2020-01 -0.008543  0.032947  0.011972  ...  0.011556  0.004658 -0.006268   \n",
      "\n",
      "             PC41      PC42      PC43      PC44      PC45      PC46      PC47  \n",
      "date                                                                           \n",
      "1973-02 -0.010304 -0.007092  0.008458  0.010323      -0.0       0.0       0.0  \n",
      "1973-03 -0.015084  0.006569  -0.00155  0.014107      -0.0      -0.0       0.0  \n",
      "1973-04 -0.005882  -0.00524  0.001282  0.011108       0.0      -0.0       0.0  \n",
      "1973-05 -0.026076 -0.004754 -0.014463  0.021661       0.0       0.0       0.0  \n",
      "1973-06  0.008113  0.004878  0.007792  0.006805       0.0       0.0      -0.0  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "2019-09 -0.000517 -0.006274  0.003448  0.001092 -0.006518 -0.004247   -0.0076  \n",
      "2019-10  0.004942  -0.00468 -0.000161  0.000391 -0.008263  0.006965  0.001521  \n",
      "2019-11 -0.001159  0.012408  0.004936  0.003515 -0.006954 -0.000451  0.000488  \n",
      "2019-12 -0.002737  0.009973  0.009825  0.004672  0.001641 -0.011689 -0.006748  \n",
      "2020-01  0.010561 -0.001968 -0.003344 -0.006232  0.000492  0.005839  0.001024  \n",
      "\n",
      "[564 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "# initialize pca model \n",
    "\n",
    "pca = PCA(n_components=len(factors))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# select our start date \n",
    "\n",
    "start_date = pd.to_datetime(\"1963-07-01\")\n",
    "\n",
    "# create an empty dataframe to store the average return for each PC from t until t-11. We need this to create the momentum signal for our strategy\n",
    "\n",
    "pc_avg_df = pd.DataFrame()\n",
    "\n",
    "# create an empty list for the pc return dataframes. These will be concated in a later stage to one large dataframe\n",
    "\n",
    "pc_return_dfs = []\n",
    "\n",
    "# create our loop set up, this is actually an expanding PCA analysis. In each iteration a new month is added to the dataset and the return is computed. \n",
    "\n",
    "for year in range(1973, 2020):\n",
    "    # the sample of the paper starts from July 1973, but we use January to June 1973 to calculate the returns in order to obtain stable means for later demeaning purposes \n",
    "    for mo in range(1,13):\n",
    "        # first we have to find the last month of the day. For this we use the calender function with inputs from the loop variables\n",
    "        last_day = calendar.monthrange(year, mo)[1]\n",
    "\n",
    "        # we select our new end_date variable for which the PCA analysis is done, also with inputs from our loop and the last_day variable\n",
    "        end_date = pd.to_datetime(f'{year}-{mo}-{last_day}')\n",
    "\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "\n",
    "        # we select the datarange from our dataset (July 1963 = start_date until our defined end_date) and we fit the model\n",
    "        pca_data = r_daily.loc[start_date:end_date]\n",
    "        scaled_data = scaler.fit_transform(pca_data)\n",
    "        pca.fit(scaled_data)\n",
    "\n",
    "        # we extract the principal components. These principal components are put in a new dataframe for later analysis. \n",
    "\n",
    "        principal_components = pca.components_\n",
    "        components_df = pd.DataFrame(data=principal_components.T, index=factors, columns=[f\"PC{i+1}\" for i in range(len(factors))])\n",
    "\n",
    "        # calculating return for month t+1. If mo = 12, then year will increment with 1. \n",
    "\n",
    "        t_plus_1_year = year + 1 if mo == 12 else year\n",
    "        t_plus_1_month = (mo % 12) + 1\n",
    "\n",
    "        # creating a datetime variable for the month t+1 and storing this in our pc_return_data variable\n",
    "\n",
    "        t_plus_1_dt =pd.to_datetime(f'{t_plus_1_year}-{t_plus_1_month}')\n",
    "        t_plus_1 = t_plus_1_dt.strftime('%Y-%m')\n",
    "\n",
    "        pc_return_data = {'date': t_plus_1}\n",
    "\n",
    "\n",
    "        # in this loop we calculate the monthly factor returns (f) using the principal components and returns\n",
    "\n",
    "        for f in range(len(factors)):\n",
    "            # select our factor and extract its principal component from principal_df and its return from r_daily for all observations in month mo \n",
    "            pc = components_df.iloc[:, f]\n",
    "            r_month = r_monthly.loc[t]\n",
    "            # multiply the principal components with the returns and sum them up to get PC factor return for month mo \n",
    "            pc_return = (pc*r_month).sum()\n",
    "\n",
    "            # place this in our dictionary for later transposing to dataframe\n",
    "\n",
    "            pc_return_data[components_df.columns[f]] = pc_return\n",
    "\n",
    "            r_pc_month_n_list = []\n",
    "            \n",
    "            # in this loop we calculate the average return of the eigenvector at time t, for the period t until t-11. We store these results in a dataframe for later use.\n",
    "\n",
    "            for n in range(0, 12):\n",
    "                # calculate the datetime for t - n\n",
    "                t_minus_n_dt = t_dt - pd.DateOffset(months=n)\n",
    "\n",
    "                # transpose it to our YYYY-MM format\n",
    "                t_minus_n = t_minus_n_dt.strftime('%Y-%m')\n",
    "\n",
    "                # select the return corresponding to our month t-n\n",
    "                r_month_n = r_monthly.shift(n).loc[t_minus_n]\n",
    "\n",
    "                # calculate the dot product for month t-n\n",
    "                pc_return_n = (pc*r_month_n).sum()\n",
    "                \n",
    "                # append this to our list to calculate the mean \n",
    "                r_pc_month_n_list.append(pc_return_n)\n",
    "            \n",
    "            # calculate the mean and append it to our average return dataframe\n",
    "            r_pc_month_mean = (np.mean(r_pc_month_n_list))\n",
    "            pc_avg_df.loc[t, f'PC{f+1}'] = r_pc_month_mean\n",
    "\n",
    "        # append the PC returns to our PC_return dataframe for later analysis \n",
    "        pc_return_df = pd.DataFrame.from_dict(pc_return_data, orient='index').T\n",
    "        pc_return_df.set_index('date', inplace=True)\n",
    "        pc_return_dfs.append(pc_return_df)\n",
    "\n",
    "r_pc = pd.concat(pc_return_dfs)\n",
    "print(r_pc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:00:46.496892Z",
     "start_time": "2024-02-26T14:59:21.287217Z"
    }
   },
   "id": "6c93e32e30ad672c",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "### demeaning and leveraging our PC returns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5de35b475e77b4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "1973-03 -0.026649  0.026649  0.026649 -0.026649 -0.026649 -0.026649  0.026649   \n",
      "1973-04  0.020729  0.040113  0.043471  0.002199 -0.000572  0.006273  0.043448   \n",
      "1973-05 -0.052914  0.036520  0.048040 -0.020672  0.012657  0.013721  0.052732   \n",
      "1973-06  0.038003 -0.038651 -0.040384 -0.011589 -0.019663 -0.016462  0.028198   \n",
      "1973-07  0.029864 -0.006144  0.017073 -0.039421 -0.018559  0.019224 -0.011339   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "2019-08  0.015596 -0.013605 -0.002106  0.023654 -0.027606  0.000796  0.032523   \n",
      "2019-09  0.018075 -0.107229 -0.023939  0.075979  0.038220 -0.035257 -0.040602   \n",
      "2019-10 -0.083752  0.017626  0.127434 -0.043139 -0.056248 -0.022306  0.057079   \n",
      "2019-11  0.012635 -0.047106  0.009422 -0.046467  0.014220  0.044328  0.061344   \n",
      "2019-12  0.048361  0.010570 -0.020369 -0.003046 -0.017808  0.033079  0.008221   \n",
      "\n",
      "              PC8       PC9      PC10  ...      PC38      PC39      PC40  \\\n",
      "1973-03  0.026649  0.026649  0.026649  ...  0.026649 -0.026649  0.026649   \n",
      "1973-04 -0.000473  0.027079  0.041097  ...  0.033116 -0.026512  0.036923   \n",
      "1973-05  0.048145 -0.009453  0.053248  ... -0.036156 -0.011396 -0.038858   \n",
      "1973-06  0.056493  0.055656  0.038713  ... -0.000697 -0.015706  0.046731   \n",
      "1973-07  0.002509  0.054225 -0.007518  ... -0.026258  0.014451 -0.039386   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "2019-08 -0.010820 -0.026200 -0.019712  ...  0.011864 -0.015378 -0.033636   \n",
      "2019-09  0.007009 -0.043945 -0.042654  ... -0.003070  0.042823 -0.095680   \n",
      "2019-10 -0.022251  0.051168  0.056389  ...  0.046795  0.027124  0.008882   \n",
      "2019-11 -0.046100  0.039051  0.044987  ...  0.023981 -0.017842  0.069807   \n",
      "2019-12  0.011059 -0.029522  0.012129  ...  0.010046 -0.018714  0.037891   \n",
      "\n",
      "             PC41      PC42      PC43      PC44      PC45      PC46      PC47  \n",
      "1973-03 -0.026649  0.026649 -0.026649  0.026649 -0.026649 -0.026649  0.000000  \n",
      "1973-04  0.037175 -0.016872 -0.010578 -0.013927  0.043426 -0.026660  0.000000  \n",
      "1973-05 -0.051444 -0.013008 -0.051245  0.054155  0.013685  0.015231  0.057066  \n",
      "1973-06  0.054471  0.036021  0.030671 -0.040775  0.015666  0.013778 -0.037541  \n",
      "1973-07  0.053740  0.008555 -0.028171 -0.010282  0.065762  0.012659  0.011935  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "2019-08  0.004883 -0.034755 -0.007318  0.000395  0.015175 -0.023355  0.003263  \n",
      "2019-09 -0.004377 -0.029150  0.017303  0.007804 -0.041668 -0.028005 -0.074222  \n",
      "2019-10  0.021210 -0.021339 -0.003476  0.002993 -0.052871  0.057923  0.010019  \n",
      "2019-11 -0.007423  0.062285  0.025876  0.024418 -0.044328  0.001009  0.000470  \n",
      "2019-12 -0.014815  0.050252  0.053960  0.032317  0.011159 -0.084969 -0.066326  \n",
      "\n",
      "[562 rows x 47 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/2ff938wn7d11yl1k3djhsp880000gn/T/ipykernel_92321/2738485239.py:35: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  lev_r_pc.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# define our start date and create an empty list for our leveraged dataframes\n",
    "start_date_dt = pd.to_datetime(\"1963-07-01\")\n",
    "start_date = start_date_dt.strftime('%Y-%m')\n",
    "lev_dfs = []\n",
    "\n",
    "for year in range(1973, 2020):\n",
    "    # as we lost one month in calculating the t+1 PC return, we start the loop from february 1973. We try to use the full year of 1973 in order to obtain stable demeaned results. Hence we will cut of our sample later from July 1973 in order to match the dataset of the original paper. \n",
    "    for mo in range(2,13) if year == 1973 else range(1, 13):\n",
    "        \n",
    "        # first we set our t variable to the current year and month from our loop\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "        \n",
    "        # we also create a t_minus_one variable, because we have to calculate the variance up to month t (so excluding month t)\n",
    "        t_minus_one_dt = t_dt - pd.DateOffset(months=1)\n",
    "        t_minus_one = t_minus_one_dt.strftime('%Y-%m')\n",
    "\n",
    "        # calculate the variance of the individual factor returns up until month t-1 \n",
    "        r_indiv_f_t = r_monthly.loc[start_date:t_minus_one]\n",
    "        var_indiv_f_t = r_indiv_f_t.var(axis=0)\n",
    "        avg_var_indiv_f_t = var_indiv_f_t.mean()\n",
    "\n",
    "        # calculate the mean and variance of the PC factors up until month t \n",
    "        r_pc_t = r_pc.loc[:t]\n",
    "        demeaned_r_pc_t = r_pc.loc[t].to_frame().T - r_pc_t.mean()\n",
    "\n",
    "        # calculate the leverage factor and multiply this with the demeaned \n",
    "\n",
    "        leverage_t = np.divide(np.sqrt(avg_var_indiv_f_t), r_pc_t.std(axis=0), where=r_pc_t.std(axis=0)!=0)\n",
    "        lev_r_pc_t = demeaned_r_pc_t * leverage_t\n",
    "        lev_df = lev_r_pc_t.loc[t].to_frame().T\n",
    "        lev_dfs.append(lev_df)\n",
    "\n",
    "lev_r_pc = pd.concat(lev_dfs)\n",
    "lev_r_pc.fillna(0, inplace=True)\n",
    "lev_r_pc_clean = lev_r_pc.drop(lev_r_pc.index[:1])\n",
    "print(lev_r_pc_clean)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:13:06.527352Z",
     "start_time": "2024-02-26T15:12:59.594681Z"
    }
   },
   "id": "60e65f621120396c",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c75202b38082ff0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### constructing the momentum strategy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b53c254e55de9a83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mom_set_1    0.010090\n",
      "mom_set_2    0.010919\n",
      "mom_set_3    0.012050\n",
      "mom_set_4    0.010571\n",
      "mom_set_5    0.009817\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# create two boolean dataframes: one for positive average returns and one for negative average returns\n",
    "positive_returns_PC = pc_avg_df > 0\n",
    "negative_returns_PC = pc_avg_df < 0\n",
    "\n",
    "# convert the boolean dataframes to integers and 0's, one for long positions and one for short positions\n",
    "long_portfolio_PC = positive_returns_PC.astype(int)\n",
    "short_portfolio_PC = negative_returns_PC.astype(int)\n",
    "\n",
    "# create the 5 subsets of PCs\n",
    "mom_1_10 = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10']\n",
    "mom_11_20 = ['PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20']\n",
    "mom_21_30 = ['PC21', 'PC22', 'PC23', 'PC24', 'PC25', 'PC26', 'PC27', 'PC28', 'PC29', 'PC30']\n",
    "mom_31_40 = ['PC31', 'PC32', 'PC33', 'PC34', 'PC35', 'PC36', 'PC37', 'PC38', 'PC39', 'PC40']\n",
    "mom_41_47 = ['PC41', 'PC42', 'PC43', 'PC44', 'PC45', 'PC46', 'PC47']\n",
    "\n",
    "# create a list of the subsets for our loop\n",
    "mom_list = [mom_1_10, mom_11_20, mom_21_30, mom_31_40, mom_41_47]\n",
    "\n",
    "# create an empty dictionary \n",
    "r_mean_set_dict = {}\n",
    "\n",
    "# create a loop where the dummy dataframe is multiplied with the leveraged PC return dataframe. We shift the portfolio indicator with one, as we need to calculate the returns of t+1. \n",
    "for i, mom in enumerate(mom_list):\n",
    "    # create the strategy: the return of the long positions minus the return of the short positions (accounting for the fact that negative short returns need to become positive\n",
    "    r_PC_set_mom = (long_portfolio_PC[mom] * lev_r_pc_clean[mom]) - (short_portfolio_PC[mom] * lev_r_pc_clean[mom])\n",
    "    # we take the mean of the returns of the 10 PC subsets\n",
    "    r_PC_set_mean = r_PC_set_mom.mean(axis=1)\n",
    "    # we append it to our dictionary\n",
    "    r_mean_set_dict[f'mom_set_{i + 1}'] = r_PC_set_mean\n",
    "\n",
    "# create the dataframe with the series of returns for each subset of PCS\n",
    "mom_strategy = pd.concat(r_mean_set_dict, axis=1)\n",
    "\n",
    "mom_strategy.index = pd.to_datetime(mom_strategy.index)\n",
    "mom_strategy.index = mom_strategy.index.strftime('%Y-%m')\n",
    "mom_strategy.dropna(inplace=True)\n",
    "\n",
    "print(mom_strategy.mean(axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:14:22.768475Z",
     "start_time": "2024-02-26T15:14:22.739005Z"
    }
   },
   "id": "d1d8716302d332df",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replicating table 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5233882ce4ca1ad2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### replicating panel A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "971881e95f3633ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# select the full sample dataframe in the paper and create the two splitted periods \n",
    "mom_strategy_full = mom_strategy.loc['1973-07':'2019-12']\n",
    "mom_strategy_1 = mom_strategy.loc['1973-07':'1996-09']\n",
    "mom_strategy_2 = mom_strategy.loc['1996-09':]\n",
    "\n",
    "print(f'the mean of every subset of PCs is:\\n')\n",
    "print(mom_strategy_full.mean(axis=0))\n",
    "\n",
    "means = mom_strategy.mean(axis=0).tolist()\n",
    "std = mom_strategy.std(axis=0).tolist()\n",
    "N = mom_strategy.shape[0]\n",
    "\n",
    "print(f'the t-statistic of every subset of PCS is:\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)\n",
    "\n",
    "print(f'the mean of every subset of PCs is (first half):\\n')\n",
    "print(mom_strategy_1.mean(axis=0))\n",
    "\n",
    "means = mom_strategy_1.mean(axis=0).tolist()\n",
    "std = mom_strategy_1.std(axis=0).tolist()\n",
    "N = mom_strategy_1.shape[0]\n",
    "\n",
    "print(f'the t-statistic of every subset of PCS is (first half):\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)\n",
    "\n",
    "print(f'the mean of every subset of PCs is (second half):\\n')\n",
    "print(mom_strategy_2.mean(axis=0))\n",
    "\n",
    "means = mom_strategy_2.mean(axis=0).tolist()\n",
    "std = mom_strategy_2.std(axis=0).tolist()\n",
    "N = mom_strategy_2.shape[0]\n",
    "\n",
    "print(f'the t-statistic of every subset of PCS is (second half):\\n')\n",
    "for m, s in zip(means, std):\n",
    "    t_statistic = m / (s / (N**0.5))\n",
    "    print(t_statistic)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77459761ad29a9d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replicating panel B and C"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "690983981c7aa12e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading data and merging datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5abbd2787c86808f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "url = \n",
    "\n",
    "ff = pd.read_stata(\"fffactors.dta\")\n",
    "\n",
    "ff.set_index('yyyymm', inplace=True)\n",
    "\n",
    "\n",
    "ff.index = pd.to_datetime(ff.index, format='%Y%m')\n",
    "\n",
    "\n",
    "ff.index = ff.index.strftime('%Y-%m')\n",
    "\n",
    "ff5 = ff[['mktrf', 'smb', 'hml', 'rmw', 'cma']].loc['1973-08':'2019-12']\n",
    "\n",
    "mom_strategy_ff5 = pd.concat([mom_strategy, ff5], axis=1)\n",
    "\n",
    "mom_strategy_ff5['P1'] = 0\n",
    "mom_strategy_ff5['P2'] = 0\n",
    "mom_strategy_ff5.loc[mom_strategy_ff5.index <= '1996-09', 'P1'] = 1\n",
    "mom_strategy_ff5.loc[mom_strategy_ff5.index >= '1996-09', 'P2'] = 1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e76edbfe897d967b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: Timestamp('1963-07-01 00:00:00')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6798\u001B[0m, in \u001B[0;36mIndex.get_slice_bound\u001B[0;34m(self, label, side)\u001B[0m\n\u001B[1;32m   6797\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 6798\u001B[0m     slc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_loc(label)\n\u001B[1;32m   6799\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: Timestamp('1963-07-01 00:00:00')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 35\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# we select the datarange from our dataset (July 1963 = start_date until our defined end_date) and we fit the model\u001B[39;00m\n\u001B[1;32m     34\u001B[0m pca_data \u001B[38;5;241m=\u001B[39m r_daily\u001B[38;5;241m.\u001B[39mloc[start_date:end_date]\n\u001B[0;32m---> 35\u001B[0m return_data \u001B[38;5;241m=\u001B[39m r_monthly\u001B[38;5;241m.\u001B[39mloc[start_date:end_date]\n\u001B[1;32m     36\u001B[0m scaled_data \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mfit_transform(pca_data)\n\u001B[1;32m     37\u001B[0m pca\u001B[38;5;241m.\u001B[39mfit(scaled_data)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1189\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[1;32m   1190\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001B[0;32m-> 1191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_axis(maybe_callable, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1411\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mslice\u001B[39m):\n\u001B[1;32m   1410\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_key(key, axis)\n\u001B[0;32m-> 1411\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_slice_axis(key, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[1;32m   1412\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_bool_indexer(key):\n\u001B[1;32m   1413\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getbool_axis(key, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1443\u001B[0m, in \u001B[0;36m_LocIndexer._get_slice_axis\u001B[0;34m(self, slice_obj, axis)\u001B[0m\n\u001B[1;32m   1440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1442\u001B[0m labels \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_get_axis(axis)\n\u001B[0;32m-> 1443\u001B[0m indexer \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mslice_indexer(slice_obj\u001B[38;5;241m.\u001B[39mstart, slice_obj\u001B[38;5;241m.\u001B[39mstop, slice_obj\u001B[38;5;241m.\u001B[39mstep)\n\u001B[1;32m   1445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(indexer, \u001B[38;5;28mslice\u001B[39m):\n\u001B[1;32m   1446\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_slice(indexer, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6662\u001B[0m, in \u001B[0;36mIndex.slice_indexer\u001B[0;34m(self, start, end, step)\u001B[0m\n\u001B[1;32m   6618\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mslice_indexer\u001B[39m(\n\u001B[1;32m   6619\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   6620\u001B[0m     start: Hashable \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   6621\u001B[0m     end: Hashable \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   6622\u001B[0m     step: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   6623\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mslice\u001B[39m:\n\u001B[1;32m   6624\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   6625\u001B[0m \u001B[38;5;124;03m    Compute the slice indexer for input labels and step.\u001B[39;00m\n\u001B[1;32m   6626\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   6660\u001B[0m \u001B[38;5;124;03m    slice(1, 3, None)\u001B[39;00m\n\u001B[1;32m   6661\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 6662\u001B[0m     start_slice, end_slice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslice_locs(start, end, step\u001B[38;5;241m=\u001B[39mstep)\n\u001B[1;32m   6664\u001B[0m     \u001B[38;5;66;03m# return a slice\u001B[39;00m\n\u001B[1;32m   6665\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_scalar(start_slice):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6879\u001B[0m, in \u001B[0;36mIndex.slice_locs\u001B[0;34m(self, start, end, step)\u001B[0m\n\u001B[1;32m   6877\u001B[0m start_slice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   6878\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 6879\u001B[0m     start_slice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_slice_bound(start, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6880\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start_slice \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   6881\u001B[0m     start_slice \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6801\u001B[0m, in \u001B[0;36mIndex.get_slice_bound\u001B[0;34m(self, label, side)\u001B[0m\n\u001B[1;32m   6799\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   6800\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 6801\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_searchsorted_monotonic(label, side)\n\u001B[1;32m   6802\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m   6803\u001B[0m         \u001B[38;5;66;03m# raise the original KeyError\u001B[39;00m\n\u001B[1;32m   6804\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m err\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6733\u001B[0m, in \u001B[0;36mIndex._searchsorted_monotonic\u001B[0;34m(self, label, side)\u001B[0m\n\u001B[1;32m   6731\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_searchsorted_monotonic\u001B[39m(\u001B[38;5;28mself\u001B[39m, label, side: Literal[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   6732\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_monotonic_increasing:\n\u001B[0;32m-> 6733\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearchsorted(label, side\u001B[38;5;241m=\u001B[39mside)\n\u001B[1;32m   6734\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_monotonic_decreasing:\n\u001B[1;32m   6735\u001B[0m         \u001B[38;5;66;03m# np.searchsorted expects ascending sort order, have to reverse\u001B[39;00m\n\u001B[1;32m   6736\u001B[0m         \u001B[38;5;66;03m# everything for it to work (element ordering, search side and\u001B[39;00m\n\u001B[1;32m   6737\u001B[0m         \u001B[38;5;66;03m# resulting value).\u001B[39;00m\n\u001B[1;32m   6738\u001B[0m         pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msearchsorted(\n\u001B[1;32m   6739\u001B[0m             label, side\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m side \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   6740\u001B[0m         )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:1352\u001B[0m, in \u001B[0;36mIndexOpsMixin.searchsorted\u001B[0;34m(self, value, side, sorter)\u001B[0m\n\u001B[1;32m   1348\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(values, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m   1349\u001B[0m     \u001B[38;5;66;03m# Going through EA.searchsorted directly improves performance GH#38083\u001B[39;00m\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m values\u001B[38;5;241m.\u001B[39msearchsorted(value, side\u001B[38;5;241m=\u001B[39mside, sorter\u001B[38;5;241m=\u001B[39msorter)\n\u001B[0;32m-> 1352\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m algorithms\u001B[38;5;241m.\u001B[39msearchsorted(\n\u001B[1;32m   1353\u001B[0m     values,\n\u001B[1;32m   1354\u001B[0m     value,\n\u001B[1;32m   1355\u001B[0m     side\u001B[38;5;241m=\u001B[39mside,\n\u001B[1;32m   1356\u001B[0m     sorter\u001B[38;5;241m=\u001B[39msorter,\n\u001B[1;32m   1357\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1329\u001B[0m, in \u001B[0;36msearchsorted\u001B[0;34m(arr, value, side, sorter)\u001B[0m\n\u001B[1;32m   1325\u001B[0m     arr \u001B[38;5;241m=\u001B[39m ensure_wrapped_if_datetimelike(arr)\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;66;03m# Argument 1 to \"searchsorted\" of \"ndarray\" has incompatible type\u001B[39;00m\n\u001B[1;32m   1328\u001B[0m \u001B[38;5;66;03m# \"Union[NumpyValueArrayLike, ExtensionArray]\"; expected \"NumpyValueArrayLike\"\u001B[39;00m\n\u001B[0;32m-> 1329\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39msearchsorted(value, side\u001B[38;5;241m=\u001B[39mside, sorter\u001B[38;5;241m=\u001B[39msorter)\n",
      "\u001B[0;31mTypeError\u001B[0m: '<' not supported between instances of 'str' and 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# initialize pca model \n",
    "\n",
    "pca = PCA(n_components=len(factors))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# select our start date \n",
    "\n",
    "start_date = pd.to_datetime(\"1963-07-01\")\n",
    "\n",
    "# create an empty dataframe to store the average return for each PC from t until t-11. We need this to create the momentum signal for our strategy\n",
    "\n",
    "pc_avg_df = pd.DataFrame()\n",
    "\n",
    "# create an empty list for the pc return dataframes. These will be concated in a later stage to one large dataframe\n",
    "\n",
    "pc_return_dfs = []\n",
    "\n",
    "# create our loop set up, this is actually an expanding PCA analysis. In each iteration a new month is added to the dataset and the return is computed. \n",
    "\n",
    "for year in range(1973, 1974):\n",
    "    # the sample of the paper starts from July 1973, but we use January to June 1973 to calculate the returns in order to obtain stable means for later demeaning purposes \n",
    "    for mo in range(1,13):\n",
    "        # first we have to find the last month of the day. For this we use the calender function with inputs from the loop variables\n",
    "        last_day = calendar.monthrange(year, mo)[1]\n",
    "\n",
    "        # we select our new end_date variable for which the PCA analysis is done, also with inputs from our loop and the last_day variable\n",
    "        end_date = pd.to_datetime(f'{year}-{mo}-{last_day}')\n",
    "\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "\n",
    "        # we select the datarange from our dataset (July 1963 = start_date until our defined end_date) and we fit the model\n",
    "        pca_data = r_daily.loc[start_date:end_date]\n",
    "        scaled_data = scaler.fit_transform(pca_data)\n",
    "        pca.fit(scaled_data)\n",
    "        \n",
    "        # we extract the principal components. These principal components are put in a new dataframe for later analysis. \n",
    "\n",
    "        principal_components = pca.components_\n",
    "        components_df = pd.DataFrame(data=principal_components.T, index=factors, columns=[f\"PC{i+1}\" for i in range(len(factors))])\n",
    "\n",
    "        # calculating return for month t+1. If mo = 12, then year will increment with 1. \n",
    "\n",
    "        t_plus_1_year = year + 1 if mo == 12 else year\n",
    "        t_plus_1_month = (mo % 12) + 1\n",
    "\n",
    "        # creating a datetime variable for the month t+1 and storing this in our pc_return_data variable\n",
    "\n",
    "        t_plus_1_dt =pd.to_datetime(f'{t_plus_1_year}-{t_plus_1_month}')\n",
    "        t_plus_1 = t_plus_1_dt.strftime('%Y-%m')\n",
    "\n",
    "        pc_return_data = {'date': t_plus_1}\n",
    "\n",
    "\n",
    "        # in this loop we calculate the monthly factor returns (f) using the principal components and returns\n",
    "\n",
    "        for f in range(len(factors)):\n",
    "            # select our factor and extract its principal component from principal_df and its return from r_daily for all observations in month mo \n",
    "            pc = components_df.iloc[:, f]\n",
    "            r_month = r_monthly.loc[t]\n",
    "            # multiply the principal components with the returns and sum them up to get PC factor return for month mo \n",
    "            pc_return = (pc*r_month).sum()\n",
    "\n",
    "            # place this in our dictionary for later transposing to dataframe\n",
    "\n",
    "            pc_return_data[components_df.columns[f]] = pc_return\n",
    "\n",
    "            r_pc_month_n_list = []\n",
    "\n",
    "            # in this loop we calculate the average return of the eigenvector at time t, for the period t until t-11. We store these results in a dataframe for later use.\n",
    "\n",
    "            for n in range(0, 12):\n",
    "                # calculate the datetime for t - n\n",
    "                t_minus_n_dt = t_dt - pd.DateOffset(months=n)\n",
    "\n",
    "                # transpose it to our YYYY-MM format\n",
    "                t_minus_n = t_minus_n_dt.strftime('%Y-%m')\n",
    "\n",
    "                # select the return corresponding to our month t-n\n",
    "                r_month_n = r_monthly.shift(n).loc[t_minus_n]\n",
    "\n",
    "                # calculate the dot product for month t-n\n",
    "                pc_return_n = (pc*r_month_n).sum()\n",
    "\n",
    "                # append this to our list to calculate the mean \n",
    "                r_pc_month_n_list.append(pc_return_n)\n",
    "\n",
    "            # calculate the mean and append it to our average return dataframe\n",
    "            r_pc_month_mean = (np.mean(r_pc_month_n_list))\n",
    "            pc_avg_df.loc[t, f'PC{f+1}'] = r_pc_month_mean\n",
    "\n",
    "        # append the PC returns to our PC_return dataframe for later analysis \n",
    "        pc_return_df = pd.DataFrame.from_dict(pc_return_data, orient='index').T\n",
    "        pc_return_df.set_index('date', inplace=True)\n",
    "        pc_return_dfs.append(pc_return_df)\n",
    "\n",
    "r_pc = pd.concat(pc_return_dfs)\n",
    "print(r_pc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:24:58.922686Z",
     "start_time": "2024-02-26T15:24:58.808577Z"
    }
   },
   "id": "3c736c6dcabd1c89",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01371338 -0.03463584 -0.00231046 ... -0.00397385 -0.00074066\n",
      "   0.00052336]\n",
      " [ 0.03500007 -0.02204684  0.00457098 ...  0.00930063  0.00136526\n",
      "  -0.00359697]\n",
      " [-0.05356279  0.04275117  0.0175099  ... -0.00051629  0.00281623\n",
      "  -0.0041991 ]\n",
      " ...\n",
      " [ 0.03559436 -0.1092548   0.01666224 ... -0.00688367 -0.00059392\n",
      "   0.000511  ]\n",
      " [ 0.20421029  0.04135745 -0.03841624 ...  0.00166796 -0.01164272\n",
      "  -0.00676036]\n",
      " [-0.0726915   0.05069172  0.06717605 ...  0.00049199  0.00583927\n",
      "   0.00102421]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but PCA was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=len(factors))\n",
    "pca_data = r_daily\n",
    "monthly_return = r_monthly\n",
    "scaled_data = scaler.fit_transform(pca_data)\n",
    "pca.fit(scaled_data)\n",
    "pca_transformed_monthly = pca.transform(monthly_return)\n",
    "print(pca_transformed_monthly)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:27:00.603642Z",
     "start_time": "2024-02-26T15:27:00.399927Z"
    }
   },
   "id": "945c1aebe8e6fba7",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f01dc087ea1a2fd8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               PC1        PC2        PC3        PC4        PC5        PC6  \\\n",
      "date                                                                        \n",
      "1973-02 -23.819597 -18.110176  -3.918106  -1.951140  30.803665  54.373116   \n",
      "1973-03 -40.839295  -4.651643   1.345406 -16.916963 -18.491059 -16.981794   \n",
      "1973-04 -27.441461  10.466830  10.094875  -6.102751   6.482101  15.221173   \n",
      "1973-05 -66.470072  16.418109  17.359363 -24.754827  24.557257  17.761599   \n",
      "1973-06 -14.907274 -18.946135  -8.718853 -19.180315   2.640592  11.187002   \n",
      "1973-07 -17.981296  -5.834703   4.174736 -25.901462   3.282226  18.966506   \n",
      "1973-08  83.773174  -6.100417   6.110009  62.934446  -9.763654  -5.781026   \n",
      "1973-09  -9.175734   0.831915   1.486356   4.791130  -2.737895  -0.185929   \n",
      "1973-10  -0.642124  27.627757  17.384921  21.869598  -6.091616 -24.626278   \n",
      "1973-11 -39.805662  10.674758   7.325617   4.736953   7.364143   2.182930   \n",
      "1973-12 -61.032372 -10.701561  20.005871 -59.559890  48.503801  63.223974   \n",
      "1974-01 -33.445359  15.400841  17.722760  25.297693  29.993792  -7.324985   \n",
      "\n",
      "               PC7        PC8        PC9       PC10  ...      PC38      PC39  \\\n",
      "date                                                 ...                       \n",
      "1973-02  -6.095624  18.134579 -26.398862  25.200109  ... -3.045208 -0.100049   \n",
      "1973-03  13.488563  10.319875  -9.478011   0.659800  ...  0.116090 -0.128183   \n",
      "1973-04  15.394531  -4.077600  -7.793180  -6.298445  ...  2.486199  0.159161   \n",
      "1973-05  42.182971   6.074789  -5.386640   4.437286  ... -2.355264  0.783441   \n",
      "1973-06  18.633360  28.122215   7.866846  -0.424076  ... -0.525903 -1.089662   \n",
      "1973-07  10.567088  14.642072   6.455528   2.491782  ... -1.879606 -0.121489   \n",
      "1973-08 -15.559098 -29.408128 -18.187227  -1.850841  ...  1.922952 -0.401085   \n",
      "1973-09  -6.453601   5.622288   3.742384   7.159941  ... -1.015711 -1.893864   \n",
      "1973-10 -26.235964  -2.518916   7.070634  -6.602344  ... -0.645010  0.383359   \n",
      "1973-11  20.662216   0.543921  17.334369  -6.770795  ... -2.516896  1.022216   \n",
      "1973-12  44.136379  16.637365  71.667091 -19.360576  ... -0.811059 -0.191727   \n",
      "1974-01  31.617433  61.033509  24.530237 -18.982756  ...  1.133269  1.766640   \n",
      "\n",
      "             PC40      PC41      PC42      PC43      PC44          PC45  \\\n",
      "date                                                                      \n",
      "1973-02 -0.778998 -0.068192 -0.826151  0.409250  0.975871 -1.172976e-14   \n",
      "1973-03  0.236736  0.636321  0.901891 -0.280308  0.739361 -4.487999e-15   \n",
      "1973-04  0.863284  1.327015 -0.919860 -0.048034  0.637996  2.803725e-15   \n",
      "1973-05 -0.170442  0.623909 -0.239807 -1.703944  0.620627  7.923808e-15   \n",
      "1973-06  1.190036 -0.004521  0.382457  0.730819  0.353818  2.308028e-16   \n",
      "1973-07 -1.032467  0.218419 -0.229981 -0.317178  0.257861  4.080916e-15   \n",
      "1973-08  1.774105 -0.502900 -0.180495 -1.840509 -1.220916  8.752222e-15   \n",
      "1973-09 -0.256809 -0.742727  1.651956  0.108424  0.636721 -2.108434e-15   \n",
      "1973-10  2.021937  0.723403 -0.525408 -0.004278  0.713341  4.360024e-15   \n",
      "1973-11 -2.228826  1.080507  0.052992  0.747781 -0.654893  4.658390e-15   \n",
      "1973-12 -0.305602 -0.298998  1.055929  0.755196  0.164156  1.928658e-01   \n",
      "1974-01 -0.652487 -0.079846 -0.526151  0.586504  0.511846 -1.416370e-01   \n",
      "\n",
      "                 PC46          PC47  \n",
      "date                                 \n",
      "1973-02 -3.277801e-15  0.000000e+00  \n",
      "1973-03 -1.574518e-15  0.000000e+00  \n",
      "1973-04  4.784617e-16  0.000000e+00  \n",
      "1973-05  0.000000e+00  3.816433e-15  \n",
      "1973-06  0.000000e+00 -2.870697e-15  \n",
      "1973-07  0.000000e+00  2.307830e-15  \n",
      "1973-08  3.144215e-14  0.000000e+00  \n",
      "1973-09  0.000000e+00 -3.632330e-16  \n",
      "1973-10  0.000000e+00  3.051164e-15  \n",
      "1973-11 -1.578069e-14  0.000000e+00  \n",
      "1973-12  0.000000e+00 -2.073074e-14  \n",
      "1974-01 -3.323471e-15  0.000000e+00  \n",
      "\n",
      "[12 rows x 47 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/sjoerd/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "# Assuming r_daily and r_monthly are your daily and monthly returns DataFrames\n",
    "# Assuming factors is a list of your factor names\n",
    "\n",
    "pca = PCA(n_components=len(factors))\n",
    "scaler = StandardScaler()\n",
    "\n",
    "start_date = pd.to_datetime(\"1963-07-01\")\n",
    "pc_avg_df = pd.DataFrame()\n",
    "pc_return_dfs = []\n",
    "\n",
    "# Loop setup for an expanding PCA analysis\n",
    "for year in range(1973, 1974):\n",
    "    for mo in range(1, 13):\n",
    "        last_day = calendar.monthrange(year, mo)[1]\n",
    "        end_date = pd.to_datetime(f'{year}-{mo}-{last_day}')\n",
    "        t_dt = pd.to_datetime(f'{year}-{mo}')\n",
    "        t = t_dt.strftime('%Y-%m')\n",
    "\n",
    "        # Fit the PCA model on the scaled daily data up to end_date\n",
    "        pca_data = r_daily.loc[start_date:end_date, factors]  # Ensure pca_data only contains factors\n",
    "        scaled_data = scaler.fit_transform(pca_data)\n",
    "        pca.fit(scaled_data)\n",
    "\n",
    "        # For calculating return for month t+1\n",
    "        t_plus_1_year = year + 1 if mo == 12 else year\n",
    "        t_plus_1_month = 1 if mo == 12 else mo + 1\n",
    "\n",
    "        t_plus_1_dt = pd.to_datetime(f'{t_plus_1_year}-{t_plus_1_month}')\n",
    "        t_plus_1 = t_plus_1_dt.strftime('%Y-%m')\n",
    "\n",
    "        # Prepare the monthly data for month t+1 for transformation\n",
    "        \n",
    "        r_month_t = r_monthly.loc[t]  # Ensure it's correctly indexed or adjusted\n",
    "        scaled_month_t = scaler.transform([r_month_t])  # Scale the data\n",
    "        pca_transformed_month_t = pca.transform(scaled_month_t)  # Transform the data\n",
    "\n",
    "            # Calculate the returns for the PC factors for month t+1\n",
    "        pc_return_data = {'date': t_plus_1}\n",
    "        for i, pc_return in enumerate(pca_transformed_month_t[0]):\n",
    "            pc_return_data[f'PC{i+1}'] = pc_return\n",
    "\n",
    "            # Append the PC returns to our dataframe for later analysis\n",
    "        pc_return_df = pd.DataFrame(pc_return_data, index=[0])\n",
    "        pc_return_df.set_index('date', inplace=True)\n",
    "        pc_return_dfs.append(pc_return_df)\n",
    "\n",
    "r_pc = pd.concat(pc_return_dfs)\n",
    "print(r_pc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:38:09.160616Z",
     "start_time": "2024-02-26T15:38:08.356551Z"
    }
   },
   "id": "2709af7d93609e5e",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cc9cd4569b3ca321"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
